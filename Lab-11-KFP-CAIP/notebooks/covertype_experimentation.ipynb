{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Experimentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings\n",
    "Make sure to update the constants to reflect your environment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'mlops-test2'\n",
    "LAB_GCS_BUCKET='gs://mlops-test2-lab-11'\n",
    "\n",
    "DATASET_LOCATION = 'US'\n",
    "CLUSTER_ZONE = 'us-central1-a'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "DATASET_ID = 'lab_11'\n",
    "SOURCE_TABLE_ID = 'covertype'\n",
    "TRAINING_TABLE_ID = 'training_split'\n",
    "VALIDATION_TABLE_ID = 'validation_split'\n",
    "TESTING_TABLE_ID = 'testing_split'\n",
    "LAB_GCS_BUCKET='gs://mlops-workshop-lab-11'\n",
    "TRAINING_FILE_PATH = LAB_GCS_BUCKET + '/datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = LAB_GCS_BUCKET + '/datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = LAB_GCS_BUCKET + '/datasets/testing/data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the source dataset \n",
    "Bring a few rows from the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3094</td>\n",
       "      <td>82</td>\n",
       "      <td>65</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3001</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1315</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7202</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3083</td>\n",
       "      <td>105</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3002</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1350</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7202</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3159</td>\n",
       "      <td>60</td>\n",
       "      <td>37</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>3045</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1177</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3158</td>\n",
       "      <td>73</td>\n",
       "      <td>62</td>\n",
       "      <td>170</td>\n",
       "      <td>-4</td>\n",
       "      <td>3042</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1187</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3147</td>\n",
       "      <td>96</td>\n",
       "      <td>59</td>\n",
       "      <td>216</td>\n",
       "      <td>-6</td>\n",
       "      <td>3037</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1209</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2506</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>201</td>\n",
       "      <td>88</td>\n",
       "      <td>655</td>\n",
       "      <td>73</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2501</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>216</td>\n",
       "      <td>81</td>\n",
       "      <td>626</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3281</td>\n",
       "      <td>38</td>\n",
       "      <td>59</td>\n",
       "      <td>150</td>\n",
       "      <td>123</td>\n",
       "      <td>3012</td>\n",
       "      <td>137</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1159</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>234</td>\n",
       "      <td>83</td>\n",
       "      <td>598</td>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "      <td>1471</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2555</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>190</td>\n",
       "      <td>135</td>\n",
       "      <td>684</td>\n",
       "      <td>67</td>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       3094      82     65                                42   \n",
       "1       3083     105     57                                 0   \n",
       "2       3159      60     37                               150   \n",
       "3       3158      73     62                               170   \n",
       "4       3147      96     59                               216   \n",
       "5       2506      13     64                               201   \n",
       "6       2501       3     63                               216   \n",
       "7       3281      38     59                               150   \n",
       "8       2500       0     62                               234   \n",
       "9       2555       3     60                               190   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                               3                             3001   \n",
       "1                               0                             3002   \n",
       "2                               0                             3045   \n",
       "3                              -4                             3042   \n",
       "4                              -6                             3037   \n",
       "5                              88                              655   \n",
       "6                              81                              626   \n",
       "7                             123                             3012   \n",
       "8                              83                              598   \n",
       "9                             135                              684   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            193               0              0   \n",
       "1            228               0              0   \n",
       "2            220               0             17   \n",
       "3            191               0              0   \n",
       "4            220               0              0   \n",
       "5             73              30              0   \n",
       "6             55              40              0   \n",
       "7            137              42              0   \n",
       "8             54              45             67   \n",
       "9             67              53             65   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points Wilderness_Area  Soil_Type  Cover_Type  \n",
       "0                                1315       Commanche       7202           2  \n",
       "1                                1350       Commanche       7202           2  \n",
       "2                                1177       Commanche       7756           2  \n",
       "3                                1187       Commanche       7756           2  \n",
       "4                                1209       Commanche       7756           2  \n",
       "5                                1470       Commanche       4703           2  \n",
       "6                                1470       Commanche       4703           2  \n",
       "7                                1159       Commanche       7756           2  \n",
       "8                                1471       Commanche       4703           2  \n",
       "9                                1470       Commanche       4703           2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = bigquery.Client(project=PROJECT_ID, location=DATASET_LOCATION)\n",
    "\n",
    "query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM `{{ source_table }}`\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID))\n",
    "df = client.query(query).to_dataframe()\n",
    "num_of_columns = len(df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows and columns in the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581012 x 13\n"
     ]
    }
   ],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT count(*)\n",
    "FROM `{{ source_table }}`\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID))\n",
    "df = client.query(query).to_dataframe()\n",
    "number_of_rows_in_full_dataset = df.iloc[0,0]\n",
    "print('{} x {}'.format(number_of_rows_in_full_dataset, num_of_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training, validation and testing splits\n",
    "Define the sampling query template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM \n",
    "  `{{ source_table }}` AS cover\n",
    "WHERE \n",
    "  MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) in {{ lots_to_select }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the sampling query job settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "dataset_ref = client.dataset(DATASET_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training split table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7efd892d16d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(1, 2, 3)')\n",
    "\n",
    "job_config.destination = dataset_ref.table(TRAINING_TABLE_ID)\n",
    "client.query(query, job_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the training split table to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Access Denied: BigQuery BigQuery: Permission denied while writing data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-70f3f996cb8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_TABLE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINING_FILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 Access Denied: BigQuery BigQuery: Permission denied while writing data."
     ]
    }
   ],
   "source": [
    "client.extract_table(dataset_ref.table(TRAINING_TABLE_ID), TRAINING_FILE_PATH).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the extracted file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat -r 0-500 {TRAINING_FILE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the validation split table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(8)')\n",
    "\n",
    "job_config.destination = dataset_ref.table(VALIDATION_TABLE_ID)\n",
    "client.query(query, job_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the validation split table to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.extract_table(dataset_ref.table(VALIDATION_TABLE_ID), VALIDATION_FILE_PATH).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the testing split table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(9)')\n",
    "\n",
    "job_config.destination = dataset_ref.table(TESTING_TABLE_ID)\n",
    "client.query(query, job_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the testing split table to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.extract_table(dataset_ref.table(TESTING_TABLE_ID), TESTING_FILE_PATH).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the `sklearn` training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "\n",
    "X_train = df_train.drop('Cover_Type', axis=1)\n",
    "y_train = df_train['Cover_Type']\n",
    "X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "y_validation = df_validation['Cover_Type']\n",
    "\n",
    "pipeline.set_params(classifier__alpha=0.001, classifier__max_iter=200)\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the hyperparameter tuning application.\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on AI Platform Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the tuning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "  if not hptune:\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "  numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points']\n",
    "    \n",
    "  categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  \n",
    "    \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  \n",
    "  if hptune:\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    # Log it with hypertune\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  if not hptune:\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package the script into a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "\n",
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the hyperparameter configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 6\n",
    "    maxParallelTrials: 3\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_iter\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          200,\n",
    "          500\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DOUBLE\n",
    "      minValue:  0.00001\n",
    "      maxValue:  0.001\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(LAB_GCS_BUCKET, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$LAB_GCS_BUCKET/$JOB_NAME \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call AI Platform Training end-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the best run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kfp",
   "language": "python",
   "name": "kfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
