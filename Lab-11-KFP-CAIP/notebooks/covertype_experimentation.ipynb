{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Experimentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings\n",
    "Make sure to update the constants to reflect your environment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'mlops-workshop'\n",
    "DATASET_LOCATION = 'US'\n",
    "CLUSTER_NAME = 'mlops-workshop-cluster'\n",
    "CLUSTER_ZONE = 'us-central1-a'\n",
    "REGION = 'us-central1'\n",
    "DATASET_ID = 'lab_12'\n",
    "SOURCE_TABLE_ID = 'covertype'\n",
    "TRAINING_TABLE_ID = 'training_split'\n",
    "VALIDATION_TABLE_ID = 'validation_split'\n",
    "TESTING_TABLE_ID = 'testing_split'\n",
    "LAB_GCS_BUCKET='gs://mlops-workshop-lab-12'\n",
    "TRAINING_FILE_PATH = LAB_GCS_BUCKET + '/datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = LAB_GCS_BUCKET + '/datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = LAB_GCS_BUCKET + '/datasets/testing/data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the source dataset \n",
    "Bring a few rows from the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3094</td>\n",
       "      <td>82</td>\n",
       "      <td>65</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3001</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1315</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7202</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3083</td>\n",
       "      <td>105</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3002</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1350</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7202</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3159</td>\n",
       "      <td>60</td>\n",
       "      <td>37</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>3045</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1177</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3158</td>\n",
       "      <td>73</td>\n",
       "      <td>62</td>\n",
       "      <td>170</td>\n",
       "      <td>-4</td>\n",
       "      <td>3042</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1187</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3147</td>\n",
       "      <td>96</td>\n",
       "      <td>59</td>\n",
       "      <td>216</td>\n",
       "      <td>-6</td>\n",
       "      <td>3037</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1209</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2506</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>201</td>\n",
       "      <td>88</td>\n",
       "      <td>655</td>\n",
       "      <td>73</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2501</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>216</td>\n",
       "      <td>81</td>\n",
       "      <td>626</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3281</td>\n",
       "      <td>38</td>\n",
       "      <td>59</td>\n",
       "      <td>150</td>\n",
       "      <td>123</td>\n",
       "      <td>3012</td>\n",
       "      <td>137</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1159</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>234</td>\n",
       "      <td>83</td>\n",
       "      <td>598</td>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "      <td>1471</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2555</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>190</td>\n",
       "      <td>135</td>\n",
       "      <td>684</td>\n",
       "      <td>67</td>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       3094      82     65                                42   \n",
       "1       3083     105     57                                 0   \n",
       "2       3159      60     37                               150   \n",
       "3       3158      73     62                               170   \n",
       "4       3147      96     59                               216   \n",
       "5       2506      13     64                               201   \n",
       "6       2501       3     63                               216   \n",
       "7       3281      38     59                               150   \n",
       "8       2500       0     62                               234   \n",
       "9       2555       3     60                               190   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                               3                             3001   \n",
       "1                               0                             3002   \n",
       "2                               0                             3045   \n",
       "3                              -4                             3042   \n",
       "4                              -6                             3037   \n",
       "5                              88                              655   \n",
       "6                              81                              626   \n",
       "7                             123                             3012   \n",
       "8                              83                              598   \n",
       "9                             135                              684   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            193               0              0   \n",
       "1            228               0              0   \n",
       "2            220               0             17   \n",
       "3            191               0              0   \n",
       "4            220               0              0   \n",
       "5             73              30              0   \n",
       "6             55              40              0   \n",
       "7            137              42              0   \n",
       "8             54              45             67   \n",
       "9             67              53             65   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points Wilderness_Area  Soil_Type  Cover_Type  \n",
       "0                                1315       Commanche       7202           2  \n",
       "1                                1350       Commanche       7202           2  \n",
       "2                                1177       Commanche       7756           2  \n",
       "3                                1187       Commanche       7756           2  \n",
       "4                                1209       Commanche       7756           2  \n",
       "5                                1470       Commanche       4703           2  \n",
       "6                                1470       Commanche       4703           2  \n",
       "7                                1159       Commanche       7756           2  \n",
       "8                                1471       Commanche       4703           2  \n",
       "9                                1470       Commanche       4703           2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = bigquery.Client(project=PROJECT_ID, location=DATASET_LOCATION)\n",
    "\n",
    "query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM `{{ source_table }}`\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID))\n",
    "df = client.query(query).to_dataframe()\n",
    "num_of_columns = len(df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows and columns in the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581012 x 13\n"
     ]
    }
   ],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT count(*)\n",
    "FROM `{{ source_table }}`\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID))\n",
    "df = client.query(query).to_dataframe()\n",
    "number_of_rows_in_full_dataset = df.iloc[0,0]\n",
    "print('{} x {}'.format(number_of_rows_in_full_dataset, num_of_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training, validation and testing splits\n",
    "Define the sampling query template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM \n",
    "  `{{ source_table }}` AS cover\n",
    "WHERE \n",
    "  MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) in {{ lots_to_select }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the sampling query job settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "dataset_ref = client.dataset(DATASET_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training split table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7f2bfe866828>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(1, 2, 3)')\n",
    "\n",
    "job_config.destination = dataset_ref.table(TRAINING_TABLE_ID)\n",
    "client.query(query, job_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the training split table to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7f2bfe8fa860>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.extract_table(dataset_ref.table(TRAINING_TABLE_ID), TRAINING_FILE_PATH).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the extracted file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevation,Aspect,Slope,Horizontal_Distance_To_Hydrology,Vertical_Distance_To_Hydrology,Horizontal_Distance_To_Roadways,Hillshade_9am,Hillshade_Noon,Hillshade_3pm,Horizontal_Distance_To_Fire_Points,Wilderness_Area,Soil_Type,Cover_Type\n",
      "3222,0,0,120,1,3207,218,237,156,1698,Rawah,7201,1\n",
      "3211,180,0,437,30,5878,219,238,157,2230,Rawah,7201,2\n",
      "3283,225,0,511,25,6031,218,238,157,631,Rawah,7201,1\n",
      "3211,0,0,418,30,5862,218,238,156,2210,Rawah,7201,2\n",
      "3135,135,0,192,5,306,219,238,156,2790,Neota,7201,1\n",
      "3068,0,0,4"
     ]
    }
   ],
   "source": [
    "!gsutil cat -r 0-500 {TRAINING_FILE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the validation split table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7f2bfe86ffd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(8)')\n",
    "\n",
    "job_config.destination = dataset_ref.table(VALIDATION_TABLE_ID)\n",
    "client.query(query, job_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the validation split table to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7f2c1a541dd8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.extract_table(dataset_ref.table(VALIDATION_TABLE_ID), VALIDATION_FILE_PATH).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the testing split table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7f2bfe87add8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(9)')\n",
    "\n",
    "job_config.destination = dataset_ref.table(TESTING_TABLE_ID)\n",
    "client.query(query, job_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the testing split table to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7f2bfe883da0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.extract_table(dataset_ref.table(TESTING_TABLE_ID), TESTING_FILE_PATH).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the `sklearn` training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7036413422453527"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "\n",
    "X_train = df_train.drop('Cover_Type', axis=1)\n",
    "y_train = df_train['Cover_Type']\n",
    "X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "y_validation = df_validation['Cover_Type']\n",
    "\n",
    "pipeline.set_params(classifier__alpha=0.001, classifier__max_iter=1000)\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the hyperparameter tuning application.\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on AI Platform Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the tuning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "  if not hptune:\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "  numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points']\n",
    "    \n",
    "  categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  \n",
    "    \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  \n",
    "  if hptune:\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    # Log it with hypertune\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  if not hptune:\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package the script into a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 5.1 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://mlops-workshop_cloudbuild/source/1576429568.5-185f0a560eb649c3b1bf1d7a7837bf23.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-workshop/builds/45134437-e6df-48ba-b5f5-38c492f099e6].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/45134437-e6df-48ba-b5f5-38c492f099e6?project=745302968357].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"45134437-e6df-48ba-b5f5-38c492f099e6\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-workshop_cloudbuild/source/1576429568.5-185f0a560eb649c3b1bf1d7a7837bf23.tgz#1576429568970317\n",
      "Copying gs://mlops-workshop_cloudbuild/source/1576429568.5-185f0a560eb649c3b1bf1d7a7837bf23.tgz#1576429568970317...\n",
      "/ [1 files][  1.5 KiB/  1.5 KiB]                                                \n",
      "Operation completed over 1 objects/1.5 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  9.728kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "35c102085707: Pulling fs layer\n",
      "251f5509d51d: Pulling fs layer\n",
      "8e829fe70a46: Pulling fs layer\n",
      "6001e1789921: Pulling fs layer\n",
      "1259902c87a2: Pulling fs layer\n",
      "83ca0edf82af: Pulling fs layer\n",
      "a459cc7a0819: Pulling fs layer\n",
      "221c4376244e: Pulling fs layer\n",
      "6be10f944cd9: Pulling fs layer\n",
      "34c517f627e3: Pulling fs layer\n",
      "8bc377099823: Pulling fs layer\n",
      "f28fcd8ca9f0: Pulling fs layer\n",
      "a5d245cced6f: Pulling fs layer\n",
      "8c6be6aa5553: Pulling fs layer\n",
      "1d7154118978: Pulling fs layer\n",
      "1df8626a77b0: Pulling fs layer\n",
      "6001e1789921: Waiting\n",
      "1259902c87a2: Waiting\n",
      "83ca0edf82af: Waiting\n",
      "a459cc7a0819: Waiting\n",
      "221c4376244e: Waiting\n",
      "6be10f944cd9: Waiting\n",
      "34c517f627e3: Waiting\n",
      "8bc377099823: Waiting\n",
      "f28fcd8ca9f0: Waiting\n",
      "a5d245cced6f: Waiting\n",
      "8c6be6aa5553: Waiting\n",
      "1d7154118978: Waiting\n",
      "1df8626a77b0: Waiting\n",
      "251f5509d51d: Verifying Checksum\n",
      "251f5509d51d: Download complete\n",
      "8e829fe70a46: Download complete\n",
      "6001e1789921: Verifying Checksum\n",
      "6001e1789921: Download complete\n",
      "35c102085707: Verifying Checksum\n",
      "35c102085707: Download complete\n",
      "a459cc7a0819: Verifying Checksum\n",
      "a459cc7a0819: Download complete\n",
      "83ca0edf82af: Verifying Checksum\n",
      "83ca0edf82af: Download complete\n",
      "6be10f944cd9: Verifying Checksum\n",
      "6be10f944cd9: Download complete\n",
      "34c517f627e3: Verifying Checksum\n",
      "34c517f627e3: Download complete\n",
      "8bc377099823: Verifying Checksum\n",
      "8bc377099823: Download complete\n",
      "f28fcd8ca9f0: Verifying Checksum\n",
      "f28fcd8ca9f0: Download complete\n",
      "a5d245cced6f: Verifying Checksum\n",
      "a5d245cced6f: Download complete\n",
      "8c6be6aa5553: Verifying Checksum\n",
      "8c6be6aa5553: Download complete\n",
      "1d7154118978: Verifying Checksum\n",
      "1d7154118978: Download complete\n",
      "1df8626a77b0: Verifying Checksum\n",
      "1df8626a77b0: Download complete\n",
      "1259902c87a2: Verifying Checksum\n",
      "1259902c87a2: Download complete\n",
      "221c4376244e: Verifying Checksum\n",
      "221c4376244e: Download complete\n",
      "35c102085707: Pull complete\n",
      "251f5509d51d: Pull complete\n",
      "8e829fe70a46: Pull complete\n",
      "6001e1789921: Pull complete\n",
      "1259902c87a2: Pull complete\n",
      "83ca0edf82af: Pull complete\n",
      "a459cc7a0819: Pull complete\n",
      "221c4376244e: Pull complete\n",
      "6be10f944cd9: Pull complete\n",
      "34c517f627e3: Pull complete\n",
      "8bc377099823: Pull complete\n",
      "f28fcd8ca9f0: Pull complete\n",
      "a5d245cced6f: Pull complete\n",
      "8c6be6aa5553: Pull complete\n",
      "1d7154118978: Pull complete\n",
      "1df8626a77b0: Pull complete\n",
      "Digest: sha256:848d51a70c3608c4acd37c3dd5a5bacef9c6a51aab5b0064daf5d4258237ef62\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 8f1066e7fc0b\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in 3bc7b511fefe\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/81/10517bad9cb33f954e393ab432165bd4ea9cef3446c5a12f85ec7546c18a/cloudml-hypertune-0.1.0.dev5.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: six in /root/miniconda3/lib/python3.7/site-packages (from fire) (1.12.0)\n",
      "Collecting termcolor\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=3e06c8d2ec6f02a7413bf6631e7a9f7a57faea7369e93d91eb800a7b21f95e4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev5-py2.py3-none-any.whl size=3926 sha256=355fb737ee0b285d2ca479379696e8ebb171e52c3f38c512205e502b34e8c928\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/c9/ed/f049e7ab403c2c3e3a7d110b9b9300cffca397fab604ae8a99\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=2d7ed8ad738a5d49a5708483c3c6b4b692146f2f9c7a60db58b912c85097dd4e\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, fire, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev5 fire-0.2.1 termcolor-1.1.0\n",
      "Removing intermediate container 3bc7b511fefe\n",
      " ---> e6144ff33e82\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 98d9edca1946\n",
      "Removing intermediate container 98d9edca1946\n",
      " ---> 43757ecb1696\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 9c8c5a2f986c\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 7429442b890a\n",
      "Removing intermediate container 7429442b890a\n",
      " ---> 82b7cd408ad7\n",
      "Successfully built 82b7cd408ad7\n",
      "Successfully tagged gcr.io/mlops-workshop/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-workshop/trainer_image:latest\n",
      "The push refers to repository [gcr.io/mlops-workshop/trainer_image]\n",
      "5d4c9b029a69: Preparing\n",
      "949fbfb9b7c7: Preparing\n",
      "c875a7956345: Preparing\n",
      "07a867e0ba2d: Preparing\n",
      "092c50747c65: Preparing\n",
      "d6fb36f9bda1: Preparing\n",
      "f36c7efe6784: Preparing\n",
      "97d733be068e: Preparing\n",
      "d0ce9f8647d3: Preparing\n",
      "fa4332f1c95c: Preparing\n",
      "cd80b8f8deac: Preparing\n",
      "104fbab0f8e2: Preparing\n",
      "4019db0181d2: Preparing\n",
      "5a78197acff6: Preparing\n",
      "804e87810c15: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "d6fb36f9bda1: Waiting\n",
      "f36c7efe6784: Waiting\n",
      "97d733be068e: Waiting\n",
      "d0ce9f8647d3: Waiting\n",
      "fa4332f1c95c: Waiting\n",
      "cd80b8f8deac: Waiting\n",
      "104fbab0f8e2: Waiting\n",
      "4019db0181d2: Waiting\n",
      "5a78197acff6: Waiting\n",
      "804e87810c15: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "092c50747c65: Layer already exists\n",
      "07a867e0ba2d: Layer already exists\n",
      "d6fb36f9bda1: Layer already exists\n",
      "f36c7efe6784: Layer already exists\n",
      "97d733be068e: Layer already exists\n",
      "d0ce9f8647d3: Layer already exists\n",
      "fa4332f1c95c: Layer already exists\n",
      "cd80b8f8deac: Layer already exists\n",
      "104fbab0f8e2: Layer already exists\n",
      "4019db0181d2: Layer already exists\n",
      "804e87810c15: Layer already exists\n",
      "5a78197acff6: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "5d4c9b029a69: Pushed\n",
      "949fbfb9b7c7: Pushed\n",
      "c875a7956345: Pushed\n",
      "latest: digest: sha256:1d58d817fc6e5037fe4baadb6e8504e64eff4c8a6b8cfe19c72919a29aa90f71 size: 4283\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                   IMAGES                                         STATUS\n",
      "45134437-e6df-48ba-b5f5-38c492f099e6  2019-12-15T17:06:09+00:00  2M48S     gs://mlops-workshop_cloudbuild/source/1576429568.5-185f0a560eb649c3b1bf1d7a7837bf23.tgz  gcr.io/mlops-workshop/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "\n",
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the hyperparameter configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 6\n",
    "    maxParallelTrials: 3\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_iter\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          500,\n",
    "          1000\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DOUBLE\n",
    "      minValue:  0.00001\n",
    "      maxValue:  0.01\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20191215_051415] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20191215_051415\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20191215_051415\n",
      "jobId: JOB_20191215_051415\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(LAB_GCS_BUCKET, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$LAB_GCS_BUCKET/$JOB_NAME \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2019-12-15T17:10:20Z'\n",
      "etag: a1Z-8V4yK7I=\n",
      "jobId: JOB_20191215_171018\n",
      "state: PREPARING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset_path=gs://mlops-workshop-lab-12/datasets/training/data.csv\n",
      "  - --validation_dataset_path=gs://mlops-workshop-lab-12/datasets/validation/data.csv\n",
      "  - --alpha\n",
      "  - '0.001'\n",
      "  - --max_iter\n",
      "  - '200'\n",
      "  - --hptune\n",
      "  - 'False'\n",
      "  jobDir: gs://mlops-workshop-lab-12/JOB_20191215_171018\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/mlops-workshop/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput: {}\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20191215_171018?project=mlops-workshop\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2FJOB_20191215_171018&project=mlops-workshop\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2019-12-15 17:10:20 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2019-12-15 17:10:20 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2019-12-15 17:10:20 +0000\tservice\t\tJob JOB_20191215_171018 is queued.\n",
      "INFO\t2019-12-15 17:10:21 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-15 17:10:22 +0000\tservice\t\tWaiting for training program to start.\n",
      "INFO\t2019-12-15 17:13:41 +0000\tmaster-replica-0\t\tCopying file://model.pkl [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-15 17:13:41 +0000\tmaster-replica-0\t\t/ [0 files][    0.0 B/  6.8 KiB]                                                \n",
      "INFO\t2019-12-15 17:13:41 +0000\tmaster-replica-0\t\t/ [1 files][  6.8 KiB/  6.8 KiB]                                                \n",
      "INFO\t2019-12-15 17:13:41 +0000\tmaster-replica-0\t\tOperation completed over 1 objects/6.8 KiB.                                      \n",
      "INFO\t2019-12-15 17:13:41 +0000\tmaster-replica-0\t\tStarting training: alpha=0.001, max_iter=200\n",
      "INFO\t2019-12-15 17:13:41 +0000\tmaster-replica-0\t\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191215_171018/model.pkl\n",
      "INFO\t2019-12-15 17:16:52 +0000\tservice\t\tJob completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call AI Platform Training end-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20191215_051415',\n",
       " 'trainingInput': {'args': ['--training_dataset_path=gs://mlops-workshop-lab-12/datasets/training/data.csv',\n",
       "   '--validation_dataset_path=gs://mlops-workshop-lab-12/datasets/validation/data.csv',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'max_iter',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [500, 1000]},\n",
       "    {'parameterName': 'alpha',\n",
       "     'minValue': 1e-05,\n",
       "     'maxValue': 0.01,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'}],\n",
       "   'maxTrials': 6,\n",
       "   'maxParallelTrials': 3,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://mlops-workshop-lab-12/JOB_20191215_051415',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/mlops-workshop/trainer_image:latest'}},\n",
       " 'createTime': '2019-12-15T05:14:22Z',\n",
       " 'startTime': '2019-12-15T05:14:26Z',\n",
       " 'endTime': '2019-12-15T05:30:51Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '6',\n",
       "  'trials': [{'trialId': '5',\n",
       "    'hyperparameters': {'max_iter': '1000', 'alpha': '0.00089571222662925725'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.7016004217902274},\n",
       "    'startTime': '2019-12-15T05:22:43.048295659Z',\n",
       "    'endTime': '2019-12-15T05:29:43Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '4',\n",
       "    'hyperparameters': {'max_iter': '500', 'alpha': '0.0023757720804214481'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6913277888327636},\n",
       "    'startTime': '2019-12-15T05:22:41.620552269Z',\n",
       "    'endTime': '2019-12-15T05:29:38Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '1',\n",
       "    'hyperparameters': {'max_iter': '1000', 'alpha': '0.0032864836966991426'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6895930064459072},\n",
       "    'startTime': '2019-12-15T05:14:31.176897656Z',\n",
       "    'endTime': '2019-12-15T05:22:32Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '3',\n",
       "    'hyperparameters': {'alpha': '0.0057839836966991423', 'max_iter': '500'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6845757436603909},\n",
       "    'startTime': '2019-12-15T05:14:31.177074005Z',\n",
       "    'endTime': '2019-12-15T05:21:32Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '6',\n",
       "    'hyperparameters': {'max_iter': '500', 'alpha': '0.0058907122266292572'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6840314982056908},\n",
       "    'startTime': '2019-12-15T05:22:43.048476252Z',\n",
       "    'endTime': '2019-12-15T05:30:13Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '2',\n",
       "    'hyperparameters': {'alpha': '0.008281483696699142', 'max_iter': '500'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6822286851369967},\n",
       "    'startTime': '2019-12-15T05:14:31.177034196Z',\n",
       "    'endTime': '2019-12-15T05:22:22Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.39,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 'ABUR+Fu4ITI='}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the best run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trialId': '5',\n",
       " 'hyperparameters': {'max_iter': '1000', 'alpha': '0.00089571222662925725'},\n",
       " 'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.7016004217902274},\n",
       " 'startTime': '2019-12-15T05:22:43.048295659Z',\n",
       " 'endTime': '2019-12-15T05:29:43Z',\n",
       " 'state': 'SUCCEEDED'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfx",
   "language": "python",
   "name": "tfx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
