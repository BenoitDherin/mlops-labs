"apiVersion": |-
  argoproj.io/v1alpha1
"kind": |-
  Workflow
"metadata":
  "annotations":
    "pipelines.kubeflow.org/pipeline_spec": |-
      {"description": "CLV Training Pipeline using BigQuery for feature engineering and Automl Tables for model training", "inputs": [{"name": "project_id"}, {"default": "WITH\n  order_summaries as (\n    SELECT\n      a.customer_id,\n      a.order_date,\n      a.order_value,\n      a.order_qty_articles\n    FROM\n    (\n      SELECT\n        customer_id,\n        order_date,\n        ROUND(SUM(unit_price * quantity), 2) AS order_value,\n        SUM(quantity) AS order_qty_articles,\n        (\n          SELECT\n            MAX(order_date)\n          FROM\n            `mlops-workshop.lab_11.transactions` tl\n          WHERE\n            tl.customer_id = t.customer_id\n        ) latest_order\n      FROM\n        `mlops-workshop.lab_11.transactions` t\n      GROUP BY\n          customer_id,\n          order_date\n    ) a\n\n    INNER JOIN (\n      -- Only customers with more than one positive order values before threshold.\n      SELECT\n        customer_id\n      FROM (\n        -- Customers and how many positive order values  before threshold.\n        SELECT\n          customer_id,\n          SUM(positive_value) cnt_positive_value\n        FROM (\n          -- Customer with whether order was positive or not at each date.\n          SELECT\n            customer_id,\n            (\n              CASE\n                WHEN SUM(unit_price * quantity) > 0 THEN 1\n                ELSE 0\n              END ) positive_value\n          FROM\n            `mlops-workshop.lab_11.transactions`\n          WHERE\n            order_date < DATE(\"2011-08-08\")\n          GROUP BY\n            customer_id,\n            order_date)\n        GROUP BY\n          customer_id )\n      WHERE\n        cnt_positive_value > 1\n      ) b\n    ON\n      a.customer_id = b.customer_id\n    --[START common_clean]\n    WHERE\n      -- Bought in the past 3 months\n      DATE_DIFF(DATE(\"2011-12-12\"), latest_order, DAY) <= 90\n      -- Make sure returns are consistent.\n      AND (\n        (order_qty_articles > 0 and order_Value > 0) OR\n        (order_qty_articles < 0 and order_Value < 0)\n      ))\n          \nSELECT\n--  tf.customer_id,\n  ROUND(tf.monetary, 2) as monetary,\n  tf.cnt_orders AS frequency,\n  tf.recency,\n  tf.T,\n  ROUND(tf.recency/cnt_orders, 2) AS time_between,\n  ROUND(tf.avg_basket_value, 2) AS avg_basket_value,\n  ROUND(tf.avg_basket_size, 2) AS avg_basket_size,\n  tf.cnt_returns,\n  -- Target calculated for overall period\n  ROUND(tt.target_monetary, 2) as target_monetary\nFROM\n  -- This SELECT uses only data before threshold to make features.\n  (\n    SELECT\n      customer_id,\n      SUM(order_value) AS monetary,\n      DATE_DIFF(MAX(order_date), MIN(order_date), DAY) AS recency,\n      DATE_DIFF(DATE('2011-08-08'), MIN(order_date), DAY) AS T,\n      COUNT(DISTINCT order_date) AS cnt_orders,\n      AVG(order_qty_articles) avg_basket_size,\n      AVG(order_value) avg_basket_value,\n      SUM(CASE\n          WHEN order_value < 1 THEN 1\n          ELSE 0 END) AS cnt_returns\n    FROM\n      order_summaries a\n    WHERE\n      order_date <= DATE('2011-08-08')\n    GROUP BY\n      customer_id) tf,\n\n  -- This SELECT uses data after threshold to calculate the target )\n  (\n    SELECT\n      customer_id,\n      SUM(order_value) target_monetary\n    FROM\n      order_summaries\n      WHERE order_date > DATE('2011-08-08')\n    GROUP BY\n      customer_id) tt\nWHERE\n  tf.customer_id = tt.customer_id\n  AND tf.monetary > 0\n  AND tf.monetary <= 15000", "name": "feature_engineering_query"}, {"default": "us-central1", "name": "aml_compute_region"}, {"default": "features", "name": "features_table_id"}, {"default": "lab_11", "name": "features_dataset_id"}, {"default": "US", "name": "features_dataset_location"}, {"default": "clv_features", "name": "aml_dataset_name"}, {"default": "target_monetary", "name": "target_column_name"}, {"default": "clv_regression", "name": "aml_model_name"}, {"default": "1000", "name": "train_budget", "type": "Integer"}, {"default": "MINIMIZE_MAE", "name": "optimization_objective"}, {"default": "mean_absolute_error", "name": "primary_metric"}, {"default": "900", "name": "deployment_threshold", "type": "Float"}], "name": "CLV Training"}
  "generateName": |-
    clv-training-
"spec":
  "arguments":
    "parameters":
    - "name": |-
        project_id
    - "name": |-
        feature_engineering_query
      "value": "WITH\n  order_summaries as (\n    SELECT\n      a.customer_id,\n \
        \     a.order_date,\n      a.order_value,\n      a.order_qty_articles\n  \
        \  FROM\n    (\n      SELECT\n        customer_id,\n        order_date,\n\
        \        ROUND(SUM(unit_price * quantity), 2) AS order_value,\n        SUM(quantity)\
        \ AS order_qty_articles,\n        (\n          SELECT\n            MAX(order_date)\n\
        \          FROM\n            `mlops-workshop.lab_11.transactions` tl\n   \
        \       WHERE\n            tl.customer_id = t.customer_id\n        ) latest_order\n\
        \      FROM\n        `mlops-workshop.lab_11.transactions` t\n      GROUP BY\n\
        \          customer_id,\n          order_date\n    ) a\n\n    INNER JOIN (\n\
        \      -- Only customers with more than one positive order values before threshold.\n\
        \      SELECT\n        customer_id\n      FROM (\n        -- Customers and\
        \ how many positive order values  before threshold.\n        SELECT\n    \
        \      customer_id,\n          SUM(positive_value) cnt_positive_value\n  \
        \      FROM (\n          -- Customer with whether order was positive or not\
        \ at each date.\n          SELECT\n            customer_id,\n            (\n\
        \              CASE\n                WHEN SUM(unit_price * quantity) > 0 THEN\
        \ 1\n                ELSE 0\n              END ) positive_value\n        \
        \  FROM\n            `mlops-workshop.lab_11.transactions`\n          WHERE\n\
        \            order_date < DATE(\"2011-08-08\")\n          GROUP BY\n     \
        \       customer_id,\n            order_date)\n        GROUP BY\n        \
        \  customer_id )\n      WHERE\n        cnt_positive_value > 1\n      ) b\n\
        \    ON\n      a.customer_id = b.customer_id\n    --[START common_clean]\n\
        \    WHERE\n      -- Bought in the past 3 months\n      DATE_DIFF(DATE(\"\
        2011-12-12\"), latest_order, DAY) <= 90\n      -- Make sure returns are consistent.\n\
        \      AND (\n        (order_qty_articles > 0 and order_Value > 0) OR\n  \
        \      (order_qty_articles < 0 and order_Value < 0)\n      ))\n          \n\
        SELECT\n--  tf.customer_id,\n  ROUND(tf.monetary, 2) as monetary,\n  tf.cnt_orders\
        \ AS frequency,\n  tf.recency,\n  tf.T,\n  ROUND(tf.recency/cnt_orders, 2)\
        \ AS time_between,\n  ROUND(tf.avg_basket_value, 2) AS avg_basket_value,\n\
        \  ROUND(tf.avg_basket_size, 2) AS avg_basket_size,\n  tf.cnt_returns,\n \
        \ -- Target calculated for overall period\n  ROUND(tt.target_monetary, 2)\
        \ as target_monetary\nFROM\n  -- This SELECT uses only data before threshold\
        \ to make features.\n  (\n    SELECT\n      customer_id,\n      SUM(order_value)\
        \ AS monetary,\n      DATE_DIFF(MAX(order_date), MIN(order_date), DAY) AS\
        \ recency,\n      DATE_DIFF(DATE('2011-08-08'), MIN(order_date), DAY) AS T,\n\
        \      COUNT(DISTINCT order_date) AS cnt_orders,\n      AVG(order_qty_articles)\
        \ avg_basket_size,\n      AVG(order_value) avg_basket_value,\n      SUM(CASE\n\
        \          WHEN order_value < 1 THEN 1\n          ELSE 0 END) AS cnt_returns\n\
        \    FROM\n      order_summaries a\n    WHERE\n      order_date <= DATE('2011-08-08')\n\
        \    GROUP BY\n      customer_id) tf,\n\n  -- This SELECT uses data after\
        \ threshold to calculate the target )\n  (\n    SELECT\n      customer_id,\n\
        \      SUM(order_value) target_monetary\n    FROM\n      order_summaries\n\
        \      WHERE order_date > DATE('2011-08-08')\n    GROUP BY\n      customer_id)\
        \ tt\nWHERE\n  tf.customer_id = tt.customer_id\n  AND tf.monetary > 0\n  AND\
        \ tf.monetary <= 15000"
    - "name": |-
        aml_compute_region
      "value": |-
        us-central1
    - "name": |-
        features_table_id
      "value": |-
        features
    - "name": |-
        features_dataset_id
      "value": |-
        lab_11
    - "name": |-
        features_dataset_location
      "value": |-
        US
    - "name": |-
        aml_dataset_name
      "value": |-
        clv_features
    - "name": |-
        target_column_name
      "value": |-
        target_monetary
    - "name": |-
        aml_model_name
      "value": |-
        clv_regression
    - "name": |-
        train_budget
      "value": |-
        1000
    - "name": |-
        optimization_objective
      "value": |-
        MINIMIZE_MAE
    - "name": |-
        primary_metric
      "value": |-
        mean_absolute_error
    - "name": |-
        deployment_threshold
      "value": |-
        900
  "entrypoint": |-
    clv-training
  "serviceAccountName": |-
    pipeline-runner
  "templates":
  - "container":
      "args":
      - |-
        --gcp-project-id
      - |-
        {{inputs.parameters.project_id}}
      - |-
        --gcp-region
      - |-
        {{inputs.parameters.aml_compute_region}}
      - |-
        --display-name
      - |-
        {{inputs.parameters.aml_dataset_name}}
      - |-
        ----output-paths
      - |-
        /tmp/outputs/dataset_path/data
      - |-
        /tmp/outputs/create_time/data
      - |-
        /tmp/outputs/dataset_id/data
      "command":
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - |
        from typing import NamedTuple

        def automl_create_dataset_for_tables(
            gcp_project_id: str,
            gcp_region: str,
            display_name: str,
            description: str = None,
            tables_dataset_metadata: dict = {},
            retry=None, #=google.api_core.gapic_v1.method.DEFAULT,
            timeout: float = None, #=google.api_core.gapic_v1.method.DEFAULT,
            metadata: dict = None,
        ) -> NamedTuple('Outputs', [('dataset_path', str), ('create_time', str), ('dataset_id', str)]):
            '''automl_create_dataset_for_tables creates an empty Dataset for AutoML tables
            '''
            import sys
            import subprocess
            subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

            import google
            from google.cloud import automl
            client = automl.AutoMlClient()

            location_path = client.location_path(gcp_project_id, gcp_region)
            dataset_dict = {
                'display_name': display_name,
                'description': description,
                'tables_dataset_metadata': tables_dataset_metadata,
            }
            dataset = client.create_dataset(
                location_path,
                dataset_dict,
                retry or google.api_core.gapic_v1.method.DEFAULT,
                timeout or google.api_core.gapic_v1.method.DEFAULT,
                metadata,
            )
            print(dataset)
            dataset_id = dataset.name.rsplit('/', 1)[-1]
            return (dataset.name, dataset.create_time, dataset_id)

        import json
        import argparse
        _missing_arg = object()
        _parser = argparse.ArgumentParser(prog='Automl create dataset for tables', description='automl_create_dataset_for_tables creates an empty Dataset for AutoML tables\n')
        _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--display-name", dest="display_name", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--description", dest="description", type=str, required=False, default=_missing_arg)
        _parser.add_argument("--tables-dataset-metadata", dest="tables_dataset_metadata", type=json.loads, required=False, default=_missing_arg)
        _parser.add_argument("--retry", dest="retry", type=str, required=False, default=_missing_arg)
        _parser.add_argument("--timeout", dest="timeout", type=float, required=False, default=_missing_arg)
        _parser.add_argument("--metadata", dest="metadata", type=json.loads, required=False, default=_missing_arg)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = automl_create_dataset_for_tables(**_parsed_args)

        if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
            _outputs = [_outputs]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(str(_outputs[idx]))
      "env":
      - "name": |-
          GOOGLE_APPLICATION_CREDENTIALS
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      - "name": |-
          CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      "image": |-
        python:3.7
      "volumeMounts":
      - "mountPath": |-
          /secret/gcp-credentials
        "name": |-
          gcp-credentials-user-gcp-sa
    "inputs":
      "parameters":
      - "name": |-
          aml_compute_region
      - "name": |-
          aml_dataset_name
      - "name": |-
          project_id
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"description": "automl_create_dataset_for_tables creates an empty Dataset for AutoML tables\n", "inputs": [{"name": "gcp_project_id", "type": "String"}, {"name": "gcp_region", "type": "String"}, {"name": "display_name", "type": "String"}, {"name": "description", "optional": true, "type": "String"}, {"default": "{}", "name": "tables_dataset_metadata", "optional": true, "type": "JsonObject"}, {"name": "retry", "optional": true}, {"name": "timeout", "optional": true, "type": "Float"}, {"name": "metadata", "optional": true, "type": "JsonObject"}], "name": "Automl create dataset for tables", "outputs": [{"name": "dataset_path", "type": "String"}, {"name": "create_time", "type": "String"}, {"name": "dataset_id", "type": "String"}]}
    "name": |-
      automl-create-dataset-for-tables
    "outputs":
      "artifacts":
      - "name": |-
          automl-create-dataset-for-tables-create_time
        "path": |-
          /tmp/outputs/create_time/data
      - "name": |-
          automl-create-dataset-for-tables-dataset_id
        "path": |-
          /tmp/outputs/dataset_id/data
      - "name": |-
          automl-create-dataset-for-tables-dataset_path
        "path": |-
          /tmp/outputs/dataset_path/data
      "parameters":
      - "name": |-
          automl-create-dataset-for-tables-dataset_id
        "valueFrom":
          "path": |-
            /tmp/outputs/dataset_id/data
      - "name": |-
          automl-create-dataset-for-tables-dataset_path
        "valueFrom":
          "path": |-
            /tmp/outputs/dataset_path/data
    "volumes":
    - "name": |-
        gcp-credentials-user-gcp-sa
      "secret":
        "secretName": |-
          user-gcp-sa
  - "container":
      "args":
      - |-
        --gcp-project-id
      - |-
        {{inputs.parameters.project_id}}
      - |-
        --gcp-region
      - |-
        {{inputs.parameters.aml_compute_region}}
      - |-
        --display-name
      - |-
        {{inputs.parameters.aml_model_name}}
      - |-
        --dataset-id
      - |-
        {{inputs.parameters.automl-create-dataset-for-tables-dataset_id}}
      - |-
        --target-column-path
      - |-
        {{inputs.parameters.automl-split-dataset-table-column-names-target_column_path}}
      - |-
        --input-feature-column-paths
      - |-
        {{inputs.parameters.automl-split-dataset-table-column-names-feature_column_paths}}
      - |-
        --optimization-objective
      - |-
        {{inputs.parameters.optimization_objective}}
      - |-
        --train-budget-milli-node-hours
      - |-
        {{inputs.parameters.train_budget}}
      - |-
        ----output-paths
      - |-
        /tmp/outputs/model_path/data
      - |-
        /tmp/outputs/model_id/data
      "command":
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - |
        from typing import NamedTuple

        def automl_create_model_for_tables(
            gcp_project_id: str,
            gcp_region: str,
            display_name: str,
            dataset_id: str,
            target_column_path: str = None,
            input_feature_column_paths: list = None,
            optimization_objective: str = 'MAXIMIZE_AU_PRC',
            train_budget_milli_node_hours: int = 1000,
        ) -> NamedTuple('Outputs', [('model_path', str), ('model_id', str)]):
            import sys
            import subprocess
            subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

            from google.cloud import automl
            client = automl.AutoMlClient()

            location_path = client.location_path(gcp_project_id, gcp_region)
            model_dict = {
                'display_name': display_name,
                'dataset_id': dataset_id,
                'tables_model_metadata': {
                    'target_column_spec': automl.types.ColumnSpec(name=target_column_path),
                    'input_feature_column_specs': [automl.types.ColumnSpec(name=path) for path in input_feature_column_paths] if input_feature_column_paths else None,
                    'optimization_objective': optimization_objective,
                    'train_budget_milli_node_hours': train_budget_milli_node_hours,
                },
            }

            create_model_response = client.create_model(location_path, model_dict)
            print('Create model operation: {}'.format(create_model_response.operation))
            result = create_model_response.result()
            print(result)
            model_name = result.name
            model_id = model_name.rsplit('/', 1)[-1]
            return (model_name, model_id)

        import json
        import argparse
        _missing_arg = object()
        _parser = argparse.ArgumentParser(prog='Automl create model for tables', description='')
        _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--display-name", dest="display_name", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--dataset-id", dest="dataset_id", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--target-column-path", dest="target_column_path", type=str, required=False, default=_missing_arg)
        _parser.add_argument("--input-feature-column-paths", dest="input_feature_column_paths", type=json.loads, required=False, default=_missing_arg)
        _parser.add_argument("--optimization-objective", dest="optimization_objective", type=str, required=False, default=_missing_arg)
        _parser.add_argument("--train-budget-milli-node-hours", dest="train_budget_milli_node_hours", type=int, required=False, default=_missing_arg)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
        _parsed_args = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = automl_create_model_for_tables(**_parsed_args)

        if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
            _outputs = [_outputs]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(str(_outputs[idx]))
      "env":
      - "name": |-
          GOOGLE_APPLICATION_CREDENTIALS
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      - "name": |-
          CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      "image": |-
        python:3.7
      "volumeMounts":
      - "mountPath": |-
          /secret/gcp-credentials
        "name": |-
          gcp-credentials-user-gcp-sa
    "inputs":
      "parameters":
      - "name": |-
          aml_compute_region
      - "name": |-
          aml_model_name
      - "name": |-
          automl-create-dataset-for-tables-dataset_id
      - "name": |-
          automl-split-dataset-table-column-names-feature_column_paths
      - "name": |-
          automl-split-dataset-table-column-names-target_column_path
      - "name": |-
          optimization_objective
      - "name": |-
          project_id
      - "name": |-
          train_budget
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "gcp_project_id", "type": "String"}, {"name": "gcp_region", "type": "String"}, {"name": "display_name", "type": "String"}, {"name": "dataset_id", "type": "String"}, {"name": "target_column_path", "optional": true, "type": "String"}, {"name": "input_feature_column_paths", "optional": true, "type": "JsonArray"}, {"default": "MAXIMIZE_AU_PRC", "name": "optimization_objective", "optional": true, "type": "String"}, {"default": "1000", "name": "train_budget_milli_node_hours", "optional": true, "type": "Integer"}], "name": "Automl create model for tables", "outputs": [{"name": "model_path", "type": "String"}, {"name": "model_id", "type": "String"}]}
    "name": |-
      automl-create-model-for-tables
    "outputs":
      "artifacts":
      - "name": |-
          automl-create-model-for-tables-model_id
        "path": |-
          /tmp/outputs/model_id/data
      - "name": |-
          automl-create-model-for-tables-model_path
        "path": |-
          /tmp/outputs/model_path/data
      "parameters":
      - "name": |-
          automl-create-model-for-tables-model_path
        "valueFrom":
          "path": |-
            /tmp/outputs/model_path/data
    "volumes":
    - "name": |-
        gcp-credentials-user-gcp-sa
      "secret":
        "secretName": |-
          user-gcp-sa
  - "container":
      "args":
      - |-
        --model-path
      - |-
        {{inputs.parameters.automl-create-model-for-tables-model_path}}
      "command":
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - |
        def automl_deploy_model(model_path: str):

            import logging
            from google.cloud import automl_v1beta1 as automl
            from google.cloud.automl_v1beta1 import enums

            logging.basicConfig(level=logging.INFO)
            client = automl.TablesClient()

            model = client.get_model(model_name=model_path)
            if model.deployment_state != enums.Model.DeploymentState.DEPLOYED:
                logging.info("Starting model deployment: {}".format(model_path))
                response = client.deploy_model(model_name=model_path)
                response.result() # Wait for operation to complete
                logging.info("Deployment completed")
            else:
                 logging.info("Model already deployed")

        import argparse
        _parser = argparse.ArgumentParser(prog='Automl deploy model', description='')
        _parser.add_argument("--model-path", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = automl_deploy_model(**_parsed_args)

        if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
            _outputs = [_outputs]

        _output_serializers = [

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      "env":
      - "name": |-
          GOOGLE_APPLICATION_CREDENTIALS
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      - "name": |-
          CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      "image": |-
        gcr.io/mlops-workshop/lab_11_components:latest
      "volumeMounts":
      - "mountPath": |-
          /secret/gcp-credentials
        "name": |-
          gcp-credentials-user-gcp-sa
    "inputs":
      "parameters":
      - "name": |-
          automl-create-model-for-tables-model_path
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "model_path", "type": "String"}], "name": "Automl deploy model", "outputs": []}
    "name": |-
      automl-deploy-model
    "volumes":
    - "name": |-
        gcp-credentials-user-gcp-sa
      "secret":
        "secretName": |-
          user-gcp-sa
  - "container":
      "args":
      - |-
        --dataset-path
      - |-
        {{inputs.parameters.automl-create-dataset-for-tables-dataset_path}}
      - |-
        --input-uri
      - |-
        {{inputs.parameters.bq-query-table_uri}}
      - |-
        ----output-paths
      - |-
        /tmp/outputs/dataset_path/data
      "command":
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - |
        from typing import NamedTuple

        def automl_import_data_from_bigquery(
            dataset_path,
            input_uri: str,
            retry=None, #=google.api_core.gapic_v1.method.DEFAULT,
            timeout=None, #=google.api_core.gapic_v1.method.DEFAULT,
            metadata: dict = None,
        ) -> NamedTuple('Outputs', [('dataset_path', str)]):
            import sys
            import subprocess
            subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

            import google
            from google.cloud import automl
            client = automl.AutoMlClient()
            input_config = {
                'bigquery_source': {
                    'input_uri': input_uri,
                },
            }
            response = client.import_data(
                dataset_path,
                input_config,
                retry or google.api_core.gapic_v1.method.DEFAULT,
                timeout or google.api_core.gapic_v1.method.DEFAULT,
                metadata,
            )
            result = response.result()
            print(result)
            metadata = response.metadata
            print(metadata)
            return (dataset_path)

        import json
        import argparse
        _missing_arg = object()
        _parser = argparse.ArgumentParser(prog='Automl import data from bigquery', description='')
        _parser.add_argument("--dataset-path", dest="dataset_path", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--input-uri", dest="input_uri", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--retry", dest="retry", type=str, required=False, default=_missing_arg)
        _parser.add_argument("--timeout", dest="timeout", type=str, required=False, default=_missing_arg)
        _parser.add_argument("--metadata", dest="metadata", type=json.loads, required=False, default=_missing_arg)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = automl_import_data_from_bigquery(**_parsed_args)

        if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
            _outputs = [_outputs]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(str(_outputs[idx]))
      "env":
      - "name": |-
          GOOGLE_APPLICATION_CREDENTIALS
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      - "name": |-
          CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      "image": |-
        python:3.7
      "volumeMounts":
      - "mountPath": |-
          /secret/gcp-credentials
        "name": |-
          gcp-credentials-user-gcp-sa
    "inputs":
      "parameters":
      - "name": |-
          automl-create-dataset-for-tables-dataset_path
      - "name": |-
          bq-query-table_uri
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "dataset_path"}, {"name": "input_uri", "type": "String"}, {"name": "retry", "optional": true}, {"name": "timeout", "optional": true}, {"name": "metadata", "optional": true, "type": "JsonObject"}], "name": "Automl import data from bigquery", "outputs": [{"name": "dataset_path", "type": "String"}]}
    "name": |-
      automl-import-data-from-bigquery
    "outputs":
      "artifacts":
      - "name": |-
          automl-import-data-from-bigquery-dataset_path
        "path": |-
          /tmp/outputs/dataset_path/data
      "parameters":
      - "name": |-
          automl-import-data-from-bigquery-dataset_path
        "valueFrom":
          "path": |-
            /tmp/outputs/dataset_path/data
    "volumes":
    - "name": |-
        gcp-credentials-user-gcp-sa
      "secret":
        "secretName": |-
          user-gcp-sa
  - "container":
      "args":
      - |-
        --model-path
      - |-
        {{inputs.parameters.automl-create-model-for-tables-model_path}}
      - |-
        --primary-metric
      - |-
        {{inputs.parameters.primary_metric}}
      - |-
        ----output-paths
      - |-
        /tmp/outputs/primary_metric/data
      - |-
        /tmp/outputs/primary_metric_value/data
      "command":
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "from typing import NamedTuple\n\ndef automl_log_regression_metrics(model_path:\
        \ str,\n               primary_metric:str) -> NamedTuple('Outputs', [('primary_metric',\
        \ str), ('primary_metric_value', float)]):\n\n    import logging\n    import\
        \ json\n    from google.cloud import automl_v1beta1 as automl\n\n    logging.basicConfig(level=logging.INFO)\n\
        \    client = automl.TablesClient()\n\n    # Retrieve evaluation metrics\n\
        \    for evaluation in client.list_model_evaluations(model_name=model_path):\n\
        \        if evaluation.regression_evaluation_metrics.ListFields():\n     \
        \       evaluation_metrics = evaluation.regression_evaluation_metrics    \
        \  \n    primary_metric_value = getattr(evaluation_metrics, primary_metric)\n\
        \n    # Write the primary metric as a KFP pipeline metric\n    metrics = {\n\
        \        'metrics': [{\n            'name': primary_metric.replace('_', '-'),\n\
        \            'numberValue': primary_metric_value\n        }]\n    }\n    with\
        \ open('/mlpipeline-metrics.json', 'w') as f:\n       json.dump(metrics, f)\n\
        \n    return (primary_metric, primary_metric_value)\n\ndef _serialize_float(float_value:\
        \ float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n\
        \    if not isinstance(float_value, (float, int)):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of float.'.format(str(float_value), str(type(float_value))))\n\
        \    return str(float_value)\n\ndef _serialize_str(str_value: str) -> str:\n\
        \    if not isinstance(str_value, str):\n        raise TypeError('Value \"\
        {}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Automl\
        \ log regression metrics', description='')\n_parser.add_argument(\"--model-path\"\
        , dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--primary-metric\", dest=\"primary_metric\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
        , dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = automl_log_regression_metrics(**_parsed_args)\n\
        \nif not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):\n\
        \    _outputs = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\
        \    _serialize_float,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "env":
      - "name": |-
          GOOGLE_APPLICATION_CREDENTIALS
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      - "name": |-
          CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      "image": |-
        gcr.io/mlops-workshop/lab_11_components:latest
      "volumeMounts":
      - "mountPath": |-
          /secret/gcp-credentials
        "name": |-
          gcp-credentials-user-gcp-sa
    "inputs":
      "parameters":
      - "name": |-
          automl-create-model-for-tables-model_path
      - "name": |-
          primary_metric
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "model_path", "type": "String"}, {"name": "primary_metric", "type": "String"}], "name": "Automl log regression metrics", "outputs": [{"name": "primary_metric", "type": "String"}, {"name": "primary_metric_value", "type": "Float"}]}
    "name": |-
      automl-log-regression-metrics
    "outputs":
      "artifacts":
      - "name": |-
          automl-log-regression-metrics-primary_metric
        "path": |-
          /tmp/outputs/primary_metric/data
      - "name": |-
          automl-log-regression-metrics-primary_metric_value
        "path": |-
          /tmp/outputs/primary_metric_value/data
      "parameters":
      - "name": |-
          automl-log-regression-metrics-primary_metric_value
        "valueFrom":
          "path": |-
            /tmp/outputs/primary_metric_value/data
    "volumes":
    - "name": |-
        gcp-credentials-user-gcp-sa
      "secret":
        "secretName": |-
          user-gcp-sa
  - "container":
      "args":
      - |-
        --dataset-path
      - |-
        {{inputs.parameters.automl-import-data-from-bigquery-dataset_path}}
      - |-
        --target-column-name
      - |-
        {{inputs.parameters.target_column_name}}
      - |-
        --table-index
      - |-
        0
      - |-
        ----output-paths
      - |-
        /tmp/outputs/target_column_path/data
      - |-
        /tmp/outputs/feature_column_paths/data
      "command":
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - |
        from typing import NamedTuple

        def automl_split_dataset_table_column_names(
            dataset_path: str,
            target_column_name: str,
            table_index: int = 0,
        ) -> NamedTuple('Outputs', [('target_column_path', str), ('feature_column_paths', list)]):
            import sys
            import subprocess
            subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

            from google.cloud import automl
            client = automl.AutoMlClient()
            list_table_specs_response = client.list_table_specs(dataset_path)
            table_specs = [s for s in list_table_specs_response]
            print('table_specs=')
            print(table_specs)
            table_spec_name = table_specs[table_index].name

            list_column_specs_response = client.list_column_specs(table_spec_name)
            column_specs = [s for s in list_column_specs_response]
            print('column_specs=')
            print(column_specs)

            target_column_spec = [s for s in column_specs if s.display_name == target_column_name][0]
            feature_column_specs = [s for s in column_specs if s.display_name != target_column_name]
            feature_column_names = [s.name for s in feature_column_specs]

            import json
            return (target_column_spec.name, json.dumps(feature_column_names))

        import argparse
        _missing_arg = object()
        _parser = argparse.ArgumentParser(prog='Automl split dataset table column names', description='')
        _parser.add_argument("--dataset-path", dest="dataset_path", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--target-column-name", dest="target_column_name", type=str, required=True, default=_missing_arg)
        _parser.add_argument("--table-index", dest="table_index", type=int, required=False, default=_missing_arg)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
        _parsed_args = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = automl_split_dataset_table_column_names(**_parsed_args)

        if not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):
            _outputs = [_outputs]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(str(_outputs[idx]))
      "env":
      - "name": |-
          GOOGLE_APPLICATION_CREDENTIALS
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      - "name": |-
          CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      "image": |-
        python:3.7
      "volumeMounts":
      - "mountPath": |-
          /secret/gcp-credentials
        "name": |-
          gcp-credentials-user-gcp-sa
    "inputs":
      "parameters":
      - "name": |-
          automl-import-data-from-bigquery-dataset_path
      - "name": |-
          target_column_name
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "dataset_path", "type": "String"}, {"name": "target_column_name", "type": "String"}, {"default": "0", "name": "table_index", "optional": true, "type": "Integer"}], "name": "Automl split dataset table column names", "outputs": [{"name": "target_column_path", "type": "String"}, {"name": "feature_column_paths", "type": "JsonArray"}]}
    "name": |-
      automl-split-dataset-table-column-names
    "outputs":
      "artifacts":
      - "name": |-
          automl-split-dataset-table-column-names-feature_column_paths
        "path": |-
          /tmp/outputs/feature_column_paths/data
      - "name": |-
          automl-split-dataset-table-column-names-target_column_path
        "path": |-
          /tmp/outputs/target_column_path/data
      "parameters":
      - "name": |-
          automl-split-dataset-table-column-names-feature_column_paths
        "valueFrom":
          "path": |-
            /tmp/outputs/feature_column_paths/data
      - "name": |-
          automl-split-dataset-table-column-names-target_column_path
        "valueFrom":
          "path": |-
            /tmp/outputs/target_column_path/data
    "volumes":
    - "name": |-
        gcp-credentials-user-gcp-sa
      "secret":
        "secretName": |-
          user-gcp-sa
  - "container":
      "args":
      - |-
        --query
      - |-
        {{inputs.parameters.feature_engineering_query}}
      - |-
        --project-id
      - |-
        {{inputs.parameters.project_id}}
      - |-
        --dataset-id
      - |-
        {{inputs.parameters.features_dataset_id}}
      - |-
        --table-id
      - |-
        {{inputs.parameters.features_table_id}}
      - |-
        --location
      - |-
        {{inputs.parameters.features_dataset_location}}
      - |-
        ----output-paths
      - |-
        /tmp/outputs/table_uri/data
      - |-
        /tmp/outputs/job_id/data
      "command":
      - |-
        python3
      - |-
        -u
      - |-
        -c
      - "from typing import NamedTuple\n\ndef bq_query(query: str, \n            \
        \ project_id:str, \n             dataset_id: str, \n             table_id:\
        \ str, \n             location: str) -> NamedTuple('Outputs', [('table_uri',\
        \ str), ('job_id', str)]):\n\n    from google.cloud import bigquery\n    from\
        \ google.api_core import exceptions\n    import logging\n    import os\n \
        \   import uuid\n\n    logging.basicConfig(level=logging.INFO)\n\n    client\
        \ = bigquery.Client(project=project_id, location=location)\n\n    job_config\
        \ = bigquery.QueryJobConfig()\n    job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n\
        \    job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n\
        \    job_id = 'query_' + os.environ.get('KFP_POD_NAME', uuid.uuid1().hex)\n\
        \n    dataset_ref = client.dataset(dataset_id)\n    try:\n        dataset\
        \ = client.get_dataset(dataset_ref)\n    except exceptions.NotFound:\n   \
        \     dataset = bigquery.Dataset(dataset_ref)\n        dataset.location =\
        \ location\n        logging.info('Creating dataset {}'.format(dataset_id))\n\
        \        client.create_dataset(dataset)\n\n    table_id = table_id if table_id\
        \ else job_id\n    table_ref = dataset_ref.table(table_id)\n    job_config.destination\
        \ = table_ref\n    logging.info('Submitting the job {}'.format(job_id))\n\
        \    query_job = client.query(query, job_config, job_id=job_id)\n    query_job.result()\
        \ # Wait for query to finish\n\n    table_uri = 'bq://{}.{}.{}'.format(project_id,\
        \ dataset_id, table_id)\n\n    return (table_uri, job_id)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Bq\
        \ query', description='')\n_parser.add_argument(\"--query\", dest=\"query\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --project-id\", dest=\"project_id\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dataset-id\", dest=\"dataset_id\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-id\", dest=\"\
        table_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --location\", dest=\"location\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = bq_query(**_parsed_args)\n\nif not hasattr(_outputs,\
        \ '__getitem__') or isinstance(_outputs, str):\n    _outputs = [_outputs]\n\
        \n_output_serializers = [\n    _serialize_str,\n    _serialize_str,\n\n]\n\
        \nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
        \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      "env":
      - "name": |-
          GOOGLE_APPLICATION_CREDENTIALS
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      - "name": |-
          CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        "value": |-
          /secret/gcp-credentials/user-gcp-sa.json
      "image": |-
        gcr.io/mlops-workshop/lab_11_components:latest
      "volumeMounts":
      - "mountPath": |-
          /secret/gcp-credentials
        "name": |-
          gcp-credentials-user-gcp-sa
    "inputs":
      "parameters":
      - "name": |-
          feature_engineering_query
      - "name": |-
          features_dataset_id
      - "name": |-
          features_dataset_location
      - "name": |-
          features_table_id
      - "name": |-
          project_id
    "metadata":
      "annotations":
        "pipelines.kubeflow.org/component_spec": |-
          {"inputs": [{"name": "query", "type": "String"}, {"name": "project_id", "type": "String"}, {"name": "dataset_id", "type": "String"}, {"name": "table_id", "type": "String"}, {"name": "location", "type": "String"}], "name": "Bq query", "outputs": [{"name": "table_uri", "type": "String"}, {"name": "job_id", "type": "String"}]}
    "name": |-
      bq-query
    "outputs":
      "artifacts":
      - "name": |-
          bq-query-job_id
        "path": |-
          /tmp/outputs/job_id/data
      - "name": |-
          bq-query-table_uri
        "path": |-
          /tmp/outputs/table_uri/data
      "parameters":
      - "name": |-
          bq-query-table_uri
        "valueFrom":
          "path": |-
            /tmp/outputs/table_uri/data
    "volumes":
    - "name": |-
        gcp-credentials-user-gcp-sa
      "secret":
        "secretName": |-
          user-gcp-sa
  - "dag":
      "tasks":
      - "arguments":
          "parameters":
          - "name": |-
              aml_compute_region
            "value": |-
              {{inputs.parameters.aml_compute_region}}
          - "name": |-
              aml_dataset_name
            "value": |-
              {{inputs.parameters.aml_dataset_name}}
          - "name": |-
              project_id
            "value": |-
              {{inputs.parameters.project_id}}
        "name": |-
          automl-create-dataset-for-tables
        "template": |-
          automl-create-dataset-for-tables
      - "arguments":
          "parameters":
          - "name": |-
              aml_compute_region
            "value": |-
              {{inputs.parameters.aml_compute_region}}
          - "name": |-
              aml_model_name
            "value": |-
              {{inputs.parameters.aml_model_name}}
          - "name": |-
              automl-create-dataset-for-tables-dataset_id
            "value": |-
              {{tasks.automl-create-dataset-for-tables.outputs.parameters.automl-create-dataset-for-tables-dataset_id}}
          - "name": |-
              automl-split-dataset-table-column-names-feature_column_paths
            "value": |-
              {{tasks.automl-split-dataset-table-column-names.outputs.parameters.automl-split-dataset-table-column-names-feature_column_paths}}
          - "name": |-
              automl-split-dataset-table-column-names-target_column_path
            "value": |-
              {{tasks.automl-split-dataset-table-column-names.outputs.parameters.automl-split-dataset-table-column-names-target_column_path}}
          - "name": |-
              optimization_objective
            "value": |-
              {{inputs.parameters.optimization_objective}}
          - "name": |-
              project_id
            "value": |-
              {{inputs.parameters.project_id}}
          - "name": |-
              train_budget
            "value": |-
              {{inputs.parameters.train_budget}}
        "dependencies":
        - |-
          automl-create-dataset-for-tables
        - |-
          automl-split-dataset-table-column-names
        "name": |-
          automl-create-model-for-tables
        "template": |-
          automl-create-model-for-tables
      - "arguments":
          "parameters":
          - "name": |-
              automl-create-dataset-for-tables-dataset_path
            "value": |-
              {{tasks.automl-create-dataset-for-tables.outputs.parameters.automl-create-dataset-for-tables-dataset_path}}
          - "name": |-
              bq-query-table_uri
            "value": |-
              {{tasks.bq-query.outputs.parameters.bq-query-table_uri}}
        "dependencies":
        - |-
          automl-create-dataset-for-tables
        - |-
          bq-query
        "name": |-
          automl-import-data-from-bigquery
        "template": |-
          automl-import-data-from-bigquery
      - "arguments":
          "parameters":
          - "name": |-
              automl-create-model-for-tables-model_path
            "value": |-
              {{tasks.automl-create-model-for-tables.outputs.parameters.automl-create-model-for-tables-model_path}}
          - "name": |-
              primary_metric
            "value": |-
              {{inputs.parameters.primary_metric}}
        "dependencies":
        - |-
          automl-create-model-for-tables
        "name": |-
          automl-log-regression-metrics
        "template": |-
          automl-log-regression-metrics
      - "arguments":
          "parameters":
          - "name": |-
              automl-import-data-from-bigquery-dataset_path
            "value": |-
              {{tasks.automl-import-data-from-bigquery.outputs.parameters.automl-import-data-from-bigquery-dataset_path}}
          - "name": |-
              target_column_name
            "value": |-
              {{inputs.parameters.target_column_name}}
        "dependencies":
        - |-
          automl-import-data-from-bigquery
        "name": |-
          automl-split-dataset-table-column-names
        "template": |-
          automl-split-dataset-table-column-names
      - "arguments":
          "parameters":
          - "name": |-
              feature_engineering_query
            "value": |-
              {{inputs.parameters.feature_engineering_query}}
          - "name": |-
              features_dataset_id
            "value": |-
              {{inputs.parameters.features_dataset_id}}
          - "name": |-
              features_dataset_location
            "value": |-
              {{inputs.parameters.features_dataset_location}}
          - "name": |-
              features_table_id
            "value": |-
              {{inputs.parameters.features_table_id}}
          - "name": |-
              project_id
            "value": |-
              {{inputs.parameters.project_id}}
        "name": |-
          bq-query
        "template": |-
          bq-query
      - "arguments":
          "parameters":
          - "name": |-
              automl-create-model-for-tables-model_path
            "value": |-
              {{tasks.automl-create-model-for-tables.outputs.parameters.automl-create-model-for-tables-model_path}}
        "dependencies":
        - |-
          automl-create-model-for-tables
        - |-
          automl-log-regression-metrics
        "name": |-
          condition-1
        "template": |-
          condition-1
        "when": |-
          {{tasks.automl-log-regression-metrics.outputs.parameters.automl-log-regression-metrics-primary_metric_value}} < {{inputs.parameters.deployment_threshold}}
    "inputs":
      "parameters":
      - "name": |-
          aml_compute_region
      - "name": |-
          aml_dataset_name
      - "name": |-
          aml_model_name
      - "name": |-
          deployment_threshold
      - "name": |-
          feature_engineering_query
      - "name": |-
          features_dataset_id
      - "name": |-
          features_dataset_location
      - "name": |-
          features_table_id
      - "name": |-
          optimization_objective
      - "name": |-
          primary_metric
      - "name": |-
          project_id
      - "name": |-
          target_column_name
      - "name": |-
          train_budget
    "name": |-
      clv-training
  - "dag":
      "tasks":
      - "arguments":
          "parameters":
          - "name": |-
              automl-create-model-for-tables-model_path
            "value": |-
              {{inputs.parameters.automl-create-model-for-tables-model_path}}
        "name": |-
          automl-deploy-model
        "template": |-
          automl-deploy-model
    "inputs":
      "parameters":
      - "name": |-
          automl-create-model-for-tables-model_path
    "name": |-
      condition-1
