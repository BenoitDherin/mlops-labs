{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestraining training and deployment of scikit-learn model with Kubeflow Pipelines and Cloud AI Platform. \n",
    "\n",
    "In this lab you develop the KFP pipeline that orchestrates BigQuery and Cloud AI Platform services to train and deploy a **scikit-learn** model. The lab uses the [Covertype Dat Set](../datasets/covertype/README.md). The model is a multi-class classification model that predicts the type of forest cover from cartographic data. \n",
    "\n",
    "The source data is in BigQuery. The pipeline uses BigQuery to prepare training and evaluation splits, AI Platform Training to run a custom container with data preprocessing and training code, and AI Platform Prediction as a deployment target. The below diagram represents the workflow orchestrated by the pipeline.\n",
    "\n",
    "![Training pipeline](../images/kfp-caip.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP: 0.1.37\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "print('KFP: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings\n",
    "Make sure to update the constants to reflect your environment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'mlops-workshop'\n",
    "DATASET_LOCATION = 'US'\n",
    "CLUSTER_NAME = 'mlops-workshop-cluster'\n",
    "CLUSTER_ZONE = 'us-central1-a'\n",
    "REGION = 'us-central1'\n",
    "DATASET_ID = 'lab_12'\n",
    "SOURCE_TABLE_ID = 'covertype'\n",
    "TRAINING_TABLE_ID = 'training_split'\n",
    "VALIDATION_TABLE_ID = 'validation_split'\n",
    "TESTING_TABLE_ID = 'testing_split'\n",
    "LAB_GCS_BUCKET='gs://mlops-workshop-lab-12'\n",
    "TRAINING_FILE_PATH = LAB_GCS_BUCKET + '/datasets/covertype_training.csv'\n",
    "VALIDATION_FILE_PATH = LAB_GCS_BUCKET + '/datasets/covertype_validation.csv'\n",
    "TESTING_FILE_PATH = LAB_GCS_BUCKET + '/datasets/covertype_testing.csv'\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/{}/components/gcp/'.format(kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the source dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(project=PROJECT_ID, location=DATASET_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3094</td>\n",
       "      <td>82</td>\n",
       "      <td>65</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3001</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1315</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7202</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3083</td>\n",
       "      <td>105</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3002</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1350</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7202</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3159</td>\n",
       "      <td>60</td>\n",
       "      <td>37</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>3045</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1177</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3158</td>\n",
       "      <td>73</td>\n",
       "      <td>62</td>\n",
       "      <td>170</td>\n",
       "      <td>-4</td>\n",
       "      <td>3042</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1187</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3147</td>\n",
       "      <td>96</td>\n",
       "      <td>59</td>\n",
       "      <td>216</td>\n",
       "      <td>-6</td>\n",
       "      <td>3037</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1209</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2506</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>201</td>\n",
       "      <td>88</td>\n",
       "      <td>655</td>\n",
       "      <td>73</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2501</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>216</td>\n",
       "      <td>81</td>\n",
       "      <td>626</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3281</td>\n",
       "      <td>38</td>\n",
       "      <td>59</td>\n",
       "      <td>150</td>\n",
       "      <td>123</td>\n",
       "      <td>3012</td>\n",
       "      <td>137</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1159</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>234</td>\n",
       "      <td>83</td>\n",
       "      <td>598</td>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "      <td>1471</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2555</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>190</td>\n",
       "      <td>135</td>\n",
       "      <td>684</td>\n",
       "      <td>67</td>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       3094      82     65                                42   \n",
       "1       3083     105     57                                 0   \n",
       "2       3159      60     37                               150   \n",
       "3       3158      73     62                               170   \n",
       "4       3147      96     59                               216   \n",
       "5       2506      13     64                               201   \n",
       "6       2501       3     63                               216   \n",
       "7       3281      38     59                               150   \n",
       "8       2500       0     62                               234   \n",
       "9       2555       3     60                               190   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                               3                             3001   \n",
       "1                               0                             3002   \n",
       "2                               0                             3045   \n",
       "3                              -4                             3042   \n",
       "4                              -6                             3037   \n",
       "5                              88                              655   \n",
       "6                              81                              626   \n",
       "7                             123                             3012   \n",
       "8                              83                              598   \n",
       "9                             135                              684   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            193               0              0   \n",
       "1            228               0              0   \n",
       "2            220               0             17   \n",
       "3            191               0              0   \n",
       "4            220               0              0   \n",
       "5             73              30              0   \n",
       "6             55              40              0   \n",
       "7            137              42              0   \n",
       "8             54              45             67   \n",
       "9             67              53             65   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points Wilderness_Area  Soil_Type  Cover_Type  \n",
       "0                                1315       Commanche       7202           2  \n",
       "1                                1350       Commanche       7202           2  \n",
       "2                                1177       Commanche       7756           2  \n",
       "3                                1187       Commanche       7756           2  \n",
       "4                                1209       Commanche       7756           2  \n",
       "5                                1470       Commanche       4703           2  \n",
       "6                                1470       Commanche       4703           2  \n",
       "7                                1159       Commanche       7756           2  \n",
       "8                                1471       Commanche       4703           2  \n",
       "9                                1470       Commanche       4703           2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM `{{ source_table }}`\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID))\n",
    "df = client.query(query).to_dataframe()\n",
    "num_of_columns = len(df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581012 x 13\n"
     ]
    }
   ],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT count(*)\n",
    "FROM `{{ source_table }}`\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID))\n",
    "df = client.query(query).to_dataframe()\n",
    "number_of_rows_in_full_dataset = df.iloc[0,0]\n",
    "print('{} x {}'.format(number_of_rows_in_full_dataset, num_of_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training, validation and testing splits\n",
    "#### Define the sampling query template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM \n",
    "  `{{ source_table }}` AS cover\n",
    "WHERE \n",
    "  MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) in {{ lots_to_select }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure query job settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "dataset_ref = client.dataset(DATASET_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7f5c2f855160>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(1, 2, 3)')\n",
    "\n",
    "job_config.destination = dataset_ref.table(TRAINING_TABLE_ID)\n",
    "client.query(query, job_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the training split to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7f5c2f82ec50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.extract_table(dataset_ref.table(TRAINING_TABLE_ID), TRAINING_FILE_PATH).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevation,Aspect,Slope,Horizontal_Distance_To_Hydrology,Vertical_Distance_To_Hydrology,Horizontal_Distance_To_Roadways,Hillshade_9am,Hillshade_Noon,Hillshade_3pm,Horizontal_Distance_To_Fire_Points,Wilderness_Area,Soil_Type,Cover_Type\n",
      "3222,0,0,120,1,3207,218,237,156,1698,Rawah,7201,1\n",
      "3211,0,0,418,30,5862,218,238,156,2210,Rawah,7201,2\n",
      "3211,90,0,30,1,5286,219,237,155,780,Rawah,7201,1\n",
      "3046,0,0,228,1,666,218,238,156,1298,Rawah,7201,1\n",
      "3211,180,0,437,30,5878,219,238,157,2230,Rawah,7201,2\n",
      "3283,225,0,511,"
     ]
    }
   ],
   "source": [
    "!gsutil cat -r 0-500 {TRAINING_FILE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7f5c2f85ccf8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(8)')\n",
    "\n",
    "job_config.destination = dataset_ref.table(VALIDATION_TABLE_ID)\n",
    "client.query(query, job_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the validation split to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7f5c2f84fcf8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.extract_table(dataset_ref.table(VALIDATION_TABLE_ID), VALIDATION_FILE_PATH).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7f5c2f862668>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(9)')\n",
    "\n",
    "job_config.destination = dataset_ref.table(TESTING_TABLE_ID)\n",
    "client.query(query, job_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the testing split to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7f5c2f85c518>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.extract_table(dataset_ref.table(TESTING_TABLE_ID), TESTING_FILE_PATH).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the `sklearn` training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7011582223582836"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "\n",
    "X_train = df_train.drop('Cover_Type', axis=1)\n",
    "y_train = df_train['Cover_Type']\n",
    "X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "y_validation = df_validation['Cover_Type']\n",
    "\n",
    "pipeline.set_params(classifier__alpha=0.001, classifier__max_iter=1000)\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a hyperparameter tuning script\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on AI Platform Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, dataset_location='US'):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "  numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points']\n",
    "    \n",
    "  categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "  y_validation = df_validation['Cover_Type']\n",
    "    \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  accuracy = pipeline.score(X_validation, y_validation)\n",
    "  print('Finished training. Model accuracy: {}'.format(accuracy))\n",
    "    \n",
    "  # Log it with hypertune\n",
    "  hpt = hypertune.HyperTune()\n",
    "  hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag='accuracy',\n",
    "    metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  model_filename = 'model.joblib'\n",
    "  joblib.dump(value=pipeline, filename=model_filename)\n",
    "  gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "  subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "  print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package the script into a docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 2.7 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://mlops-workshop_cloudbuild/source/1576184362.86-fc356da96e2e4533a2331d2e07c2b3f8.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-workshop/builds/6a1f49ca-ce78-4da8-b2e3-af801846d370].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/6a1f49ca-ce78-4da8-b2e3-af801846d370?project=745302968357].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6a1f49ca-ce78-4da8-b2e3-af801846d370\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-workshop_cloudbuild/source/1576184362.86-fc356da96e2e4533a2331d2e07c2b3f8.tgz#1576184363357285\n",
      "Copying gs://mlops-workshop_cloudbuild/source/1576184362.86-fc356da96e2e4533a2331d2e07c2b3f8.tgz#1576184363357285...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "35c102085707: Pulling fs layer\n",
      "251f5509d51d: Pulling fs layer\n",
      "8e829fe70a46: Pulling fs layer\n",
      "6001e1789921: Pulling fs layer\n",
      "1259902c87a2: Pulling fs layer\n",
      "83ca0edf82af: Pulling fs layer\n",
      "a459cc7a0819: Pulling fs layer\n",
      "221c4376244e: Pulling fs layer\n",
      "6be10f944cd9: Pulling fs layer\n",
      "34c517f627e3: Pulling fs layer\n",
      "8bc377099823: Pulling fs layer\n",
      "f28fcd8ca9f0: Pulling fs layer\n",
      "a5d245cced6f: Pulling fs layer\n",
      "8c6be6aa5553: Pulling fs layer\n",
      "1d7154118978: Pulling fs layer\n",
      "1df8626a77b0: Pulling fs layer\n",
      "6001e1789921: Waiting\n",
      "1259902c87a2: Waiting\n",
      "83ca0edf82af: Waiting\n",
      "a459cc7a0819: Waiting\n",
      "221c4376244e: Waiting\n",
      "34c517f627e3: Waiting\n",
      "8bc377099823: Waiting\n",
      "f28fcd8ca9f0: Waiting\n",
      "a5d245cced6f: Waiting\n",
      "8c6be6aa5553: Waiting\n",
      "1d7154118978: Waiting\n",
      "1df8626a77b0: Waiting\n",
      "6be10f944cd9: Waiting\n",
      "8e829fe70a46: Download complete\n",
      "251f5509d51d: Verifying Checksum\n",
      "251f5509d51d: Download complete\n",
      "6001e1789921: Verifying Checksum\n",
      "6001e1789921: Download complete\n",
      "35c102085707: Verifying Checksum\n",
      "35c102085707: Download complete\n",
      "a459cc7a0819: Verifying Checksum\n",
      "a459cc7a0819: Download complete\n",
      "83ca0edf82af: Verifying Checksum\n",
      "83ca0edf82af: Download complete\n",
      "6be10f944cd9: Verifying Checksum\n",
      "6be10f944cd9: Download complete\n",
      "34c517f627e3: Verifying Checksum\n",
      "34c517f627e3: Download complete\n",
      "8bc377099823: Verifying Checksum\n",
      "8bc377099823: Download complete\n",
      "f28fcd8ca9f0: Verifying Checksum\n",
      "f28fcd8ca9f0: Download complete\n",
      "a5d245cced6f: Verifying Checksum\n",
      "a5d245cced6f: Download complete\n",
      "8c6be6aa5553: Verifying Checksum\n",
      "8c6be6aa5553: Download complete\n",
      "1d7154118978: Verifying Checksum\n",
      "1d7154118978: Download complete\n",
      "1df8626a77b0: Verifying Checksum\n",
      "1df8626a77b0: Download complete\n",
      "1259902c87a2: Verifying Checksum\n",
      "1259902c87a2: Download complete\n",
      "35c102085707: Pull complete\n",
      "221c4376244e: Verifying Checksum\n",
      "221c4376244e: Download complete\n",
      "251f5509d51d: Pull complete\n",
      "8e829fe70a46: Pull complete\n",
      "6001e1789921: Pull complete\n",
      "1259902c87a2: Pull complete\n",
      "83ca0edf82af: Pull complete\n",
      "a459cc7a0819: Pull complete\n",
      "221c4376244e: Pull complete\n",
      "6be10f944cd9: Pull complete\n",
      "34c517f627e3: Pull complete\n",
      "8bc377099823: Pull complete\n",
      "f28fcd8ca9f0: Pull complete\n",
      "a5d245cced6f: Pull complete\n",
      "8c6be6aa5553: Pull complete\n",
      "1d7154118978: Pull complete\n",
      "1df8626a77b0: Pull complete\n",
      "Digest: sha256:848d51a70c3608c4acd37c3dd5a5bacef9c6a51aab5b0064daf5d4258237ef62\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 8f1066e7fc0b\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in 1ffb6ebf4133\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/81/10517bad9cb33f954e393ab432165bd4ea9cef3446c5a12f85ec7546c18a/cloudml-hypertune-0.1.0.dev5.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: six in /root/miniconda3/lib/python3.7/site-packages (from fire) (1.12.0)\n",
      "Collecting termcolor\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=5dc2f403969072d130e2f8facd97f13d9568862620def3c2885bcdfe3f06039e\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev5-py2.py3-none-any.whl size=3926 sha256=328764e5234269948e1077fe5ebd4bb5a574dc60caa8f443b5c000164415947a\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/c9/ed/f049e7ab403c2c3e3a7d110b9b9300cffca397fab604ae8a99\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=2f0c2f5bb5a47f443f55f3dc9db821225c1736b597872d408b545ff2c97f6e3c\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, fire, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev5 fire-0.2.1 termcolor-1.1.0\n",
      "Removing intermediate container 1ffb6ebf4133\n",
      " ---> 7cc97b45ac4f\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 89e8c24557d9\n",
      "Removing intermediate container 89e8c24557d9\n",
      " ---> 56f17907d3ea\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> e67b3115eb05\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in f716a94bb420\n",
      "Removing intermediate container f716a94bb420\n",
      " ---> ef142055ca8f\n",
      "Successfully built ef142055ca8f\n",
      "Successfully tagged gcr.io/mlops-workshop/covertype_trainer:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-workshop/covertype_trainer:latest\n",
      "The push refers to repository [gcr.io/mlops-workshop/covertype_trainer]\n",
      "b0615f3ffcc4: Preparing\n",
      "a64727d7692d: Preparing\n",
      "83546ac3d67b: Preparing\n",
      "07a867e0ba2d: Preparing\n",
      "092c50747c65: Preparing\n",
      "d6fb36f9bda1: Preparing\n",
      "f36c7efe6784: Preparing\n",
      "97d733be068e: Preparing\n",
      "d0ce9f8647d3: Preparing\n",
      "fa4332f1c95c: Preparing\n",
      "cd80b8f8deac: Preparing\n",
      "104fbab0f8e2: Preparing\n",
      "4019db0181d2: Preparing\n",
      "5a78197acff6: Preparing\n",
      "804e87810c15: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "d6fb36f9bda1: Waiting\n",
      "f36c7efe6784: Waiting\n",
      "97d733be068e: Waiting\n",
      "d0ce9f8647d3: Waiting\n",
      "fa4332f1c95c: Waiting\n",
      "cd80b8f8deac: Waiting\n",
      "104fbab0f8e2: Waiting\n",
      "4019db0181d2: Waiting\n",
      "5a78197acff6: Waiting\n",
      "804e87810c15: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "07a867e0ba2d: Layer already exists\n",
      "092c50747c65: Layer already exists\n",
      "d6fb36f9bda1: Layer already exists\n",
      "f36c7efe6784: Layer already exists\n",
      "97d733be068e: Layer already exists\n",
      "d0ce9f8647d3: Layer already exists\n",
      "fa4332f1c95c: Layer already exists\n",
      "cd80b8f8deac: Layer already exists\n",
      "104fbab0f8e2: Layer already exists\n",
      "4019db0181d2: Layer already exists\n",
      "804e87810c15: Layer already exists\n",
      "5a78197acff6: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "a64727d7692d: Pushed\n",
      "83546ac3d67b: Pushed\n",
      "b0615f3ffcc4: Pushed\n",
      "latest: digest: sha256:31d85fc374fbeea83aa7fbb4a985140ad84658775ced7c7b4d19569d584d3c69 size: 4283\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                    IMAGES                                             STATUS\n",
      "6a1f49ca-ce78-4da8-b2e3-af801846d370  2019-12-12T20:59:23+00:00  2M42S     gs://mlops-workshop_cloudbuild/source/1576184362.86-fc356da96e2e4533a2331d2e07c2b3f8.tgz  gcr.io/mlops-workshop/covertype_trainer (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME='covertype_trainer'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "\n",
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create hyperparameter configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 9\n",
    "    maxParallelTrials: 3\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_iter\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          500,\n",
    "          1000\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DOUBLE\n",
    "      minValue:  0.00001\n",
    "      maxValue:  0.01\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20191212_210301] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20191212_210301\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20191212_210301\n",
      "jobId: JOB_20191212_210301\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME='covertype_trainer'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(LAB_GCS_BUCKET, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$LAB_GCS_BUCKET/$JOB_NAME \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2019-12-12T21:03:04Z'\n",
      "etag: 2NCS6Jj5Ckg=\n",
      "jobId: JOB_20191212_210301\n",
      "state: PREPARING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset_path=gs://mlops-workshop-lab-12/datasets/covertype_training.csv\n",
      "  - --validation_dataset_path=gs://mlops-workshop-lab-12/datasets/covertype_validation.csv\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 3\n",
      "    maxTrials: 10\n",
      "    params:\n",
      "    - discreteValues:\n",
      "      - 500.0\n",
      "      - 1000.0\n",
      "      parameterName: max_iter\n",
      "      type: DISCRETE\n",
      "    - maxValue: 0.01\n",
      "      minValue: 1e-05\n",
      "      parameterName: alpha\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "  jobDir: gs://mlops-workshop-lab-12/JOB_20191212_210301\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/mlops-workshop/covertype_trainer:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20191212_210301?project=mlops-workshop\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2FJOB_20191212_210301&project=mlops-workshop\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2019-12-12 21:03:03 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2019-12-12 21:03:04 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2019-12-12 21:03:04 +0000\tservice\t\tJob JOB_20191212_210301 is queued.\n",
      "INFO\t2019-12-12 21:03:13 +0000\tservice\t3\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:03:13 +0000\tservice\t1\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:03:13 +0000\tservice\t2\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:03:16 +0000\tservice\t1\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:03:16 +0000\tservice\t2\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:03:16 +0000\tservice\t3\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:06:24 +0000\tmaster-replica-0\t3\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:06:24 +0000\tmaster-replica-0\t3\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:06:24 +0000\tmaster-replica-0\t3\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:06:24 +0000\tmaster-replica-0\t3\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:06:24 +0000\tmaster-replica-0\t3\tStarting training: alpha=0.0034299265074729923, max_iter=500\n",
      "INFO\t2019-12-12 21:06:24 +0000\tmaster-replica-0\t3\tFinished training. Model accuracy: 0.6885555385478851\n",
      "INFO\t2019-12-12 21:06:24 +0000\tmaster-replica-0\t3\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/3/model.joblib\n",
      "INFO\t2019-12-12 21:06:25 +0000\tmaster-replica-0\t2\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:06:25 +0000\tmaster-replica-0\t2\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:06:25 +0000\tmaster-replica-0\t2\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:06:25 +0000\tmaster-replica-0\t2\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:06:25 +0000\tmaster-replica-0\t2\tStarting training: alpha=0.008424926507472992, max_iter=500\n",
      "INFO\t2019-12-12 21:06:25 +0000\tmaster-replica-0\t2\tFinished training. Model accuracy: 0.6820075854210249\n",
      "INFO\t2019-12-12 21:06:25 +0000\tmaster-replica-0\t2\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/2/model.joblib\n",
      "INFO\t2019-12-12 21:06:30 +0000\tmaster-replica-0\t1\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:06:30 +0000\tmaster-replica-0\t1\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:06:30 +0000\tmaster-replica-0\t1\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:06:30 +0000\tmaster-replica-0\t1\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:06:30 +0000\tmaster-replica-0\t1\tStarting training: alpha=0.006481509778499603, max_iter=1000\n",
      "INFO\t2019-12-12 21:06:30 +0000\tmaster-replica-0\t1\tFinished training. Model accuracy: 0.6840825212170689\n",
      "INFO\t2019-12-12 21:06:30 +0000\tmaster-replica-0\t1\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/1/model.joblib\n",
      "INFO\t2019-12-12 21:10:34 +0000\tservice\t3\tJob completed successfully.\n",
      "INFO\t2019-12-12 21:10:35 +0000\tservice\t1\tJob completed successfully.\n",
      "INFO\t2019-12-12 21:10:46 +0000\tservice\t2\tJob completed successfully.\n",
      "INFO\t2019-12-12 21:12:00 +0000\tservice\t5\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:12:00 +0000\tservice\t6\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:12:00 +0000\tservice\t4\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:12:03 +0000\tservice\t4\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:12:03 +0000\tservice\t5\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:12:03 +0000\tservice\t6\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:15:09 +0000\tmaster-replica-0\t5\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:15:09 +0000\tmaster-replica-0\t5\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:15:09 +0000\tmaster-replica-0\t5\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:15:09 +0000\tmaster-replica-0\t5\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:15:09 +0000\tmaster-replica-0\t5\tStarting training: alpha=0.007835878388881683, max_iter=500\n",
      "INFO\t2019-12-12 21:15:09 +0000\tmaster-replica-0\t5\tFinished training. Model accuracy: 0.6832151300236406\n",
      "INFO\t2019-12-12 21:15:09 +0000\tmaster-replica-0\t5\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/5/model.joblib\n",
      "INFO\t2019-12-12 21:15:10 +0000\tmaster-replica-0\t6\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:15:10 +0000\tmaster-replica-0\t6\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:15:10 +0000\tmaster-replica-0\t6\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:15:10 +0000\tmaster-replica-0\t6\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:15:10 +0000\tmaster-replica-0\t6\tStarting training: alpha=0.005338378388881683, max_iter=500\n",
      "INFO\t2019-12-12 21:15:10 +0000\tmaster-replica-0\t6\tFinished training. Model accuracy: 0.6855281732061159\n",
      "INFO\t2019-12-12 21:15:10 +0000\tmaster-replica-0\t6\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/6/model.joblib\n",
      "INFO\t2019-12-12 21:15:12 +0000\tmaster-replica-0\t4\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:15:12 +0000\tmaster-replica-0\t4\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:15:12 +0000\tmaster-replica-0\t4\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:15:12 +0000\tmaster-replica-0\t4\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:15:12 +0000\tmaster-replica-0\t4\tStarting training: alpha=0.0028408783888816836, max_iter=1000\n",
      "INFO\t2019-12-12 21:15:12 +0000\tmaster-replica-0\t4\tFinished training. Model accuracy: 0.6910046430940354\n",
      "INFO\t2019-12-12 21:15:12 +0000\tmaster-replica-0\t4\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/4/model.joblib\n",
      "INFO\t2019-12-12 21:18:31 +0000\tservice\t5\tJob completed successfully.\n",
      "INFO\t2019-12-12 21:19:01 +0000\tservice\t4\tJob completed successfully.\n",
      "INFO\t2019-12-12 21:19:31 +0000\tservice\t6\tJob completed successfully.\n",
      "INFO\t2019-12-12 21:19:47 +0000\tservice\t7\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:19:49 +0000\tservice\t7\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:19:50 +0000\tservice\t8\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:19:50 +0000\tservice\t9\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:19:52 +0000\tservice\t8\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:19:52 +0000\tservice\t9\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:23:02 +0000\tmaster-replica-0\t7\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:23:02 +0000\tmaster-replica-0\t7\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:23:02 +0000\tmaster-replica-0\t7\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:23:02 +0000\tmaster-replica-0\t7\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:23:02 +0000\tmaster-replica-0\t7\tStarting training: alpha=0.0022987899974855593, max_iter=1000\n",
      "INFO\t2019-12-12 21:23:02 +0000\tmaster-replica-0\t7\tFinished training. Model accuracy: 0.693011548208242\n",
      "INFO\t2019-12-12 21:23:02 +0000\tmaster-replica-0\t7\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/7/model.joblib\n",
      "INFO\t2019-12-12 21:23:11 +0000\tmaster-replica-0\t9\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:23:11 +0000\tmaster-replica-0\t9\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:23:11 +0000\tmaster-replica-0\t9\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:23:11 +0000\tmaster-replica-0\t9\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:23:11 +0000\tmaster-replica-0\t9\tStarting training: alpha=0.0023256017517083918, max_iter=500\n",
      "INFO\t2019-12-12 21:23:11 +0000\tmaster-replica-0\t9\tFinished training. Model accuracy: 0.693691855026617\n",
      "INFO\t2019-12-12 21:23:11 +0000\tmaster-replica-0\t9\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/9/model.joblib\n",
      "INFO\t2019-12-12 21:23:13 +0000\tmaster-replica-0\t8\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:23:13 +0000\tmaster-replica-0\t8\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:23:13 +0000\tmaster-replica-0\t8\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:23:13 +0000\tmaster-replica-0\t8\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:23:13 +0000\tmaster-replica-0\t8\tStarting training: alpha=0.003311674004083134, max_iter=1000\n",
      "INFO\t2019-12-12 21:23:13 +0000\tmaster-replica-0\t8\tFinished training. Model accuracy: 0.6878582240590506\n",
      "INFO\t2019-12-12 21:23:13 +0000\tmaster-replica-0\t8\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/8/model.joblib\n",
      "INFO\t2019-12-12 21:26:41 +0000\tservice\t8\tJob completed successfully.\n",
      "INFO\t2019-12-12 21:26:51 +0000\tservice\t9\tJob completed successfully.\n",
      "INFO\t2019-12-12 21:27:08 +0000\tservice\t7\tJob completed successfully.\n",
      "INFO\t2019-12-12 21:27:33 +0000\tservice\t10\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-12 21:27:37 +0000\tservice\t10\tWaiting for training program to start.\n",
      "INFO\t2019-12-12 21:30:52 +0000\tmaster-replica-0\t10\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-12 21:30:52 +0000\tmaster-replica-0\t10\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:30:52 +0000\tmaster-replica-0\t10\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-12 21:30:52 +0000\tmaster-replica-0\t10\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-12 21:30:52 +0000\tmaster-replica-0\t10\tStarting training: alpha=0.0018003200332565434, max_iter=500\n",
      "INFO\t2019-12-12 21:30:52 +0000\tmaster-replica-0\t10\tFinished training. Model accuracy: 0.6969913430957362\n",
      "INFO\t2019-12-12 21:30:52 +0000\tmaster-replica-0\t10\tSaved model in: gs://mlops-workshop-lab-12/JOB_20191212_210301/10/model.joblib\n",
      "INFO\t2019-12-12 21:35:07 +0000\tservice\t10\tJob completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = discovery.build('ml', 'v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20191212_210301',\n",
       " 'trainingInput': {'args': ['--training_dataset_path=gs://mlops-workshop-lab-12/datasets/covertype_training.csv',\n",
       "   '--validation_dataset_path=gs://mlops-workshop-lab-12/datasets/covertype_validation.csv'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'max_iter',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [500, 1000]},\n",
       "    {'parameterName': 'alpha',\n",
       "     'minValue': 1e-05,\n",
       "     'maxValue': 0.01,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'}],\n",
       "   'maxTrials': 10,\n",
       "   'maxParallelTrials': 3,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://mlops-workshop-lab-12/JOB_20191212_210301',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/mlops-workshop/covertype_trainer:latest'}},\n",
       " 'createTime': '2019-12-12T21:03:04Z',\n",
       " 'startTime': '2019-12-12T21:03:08Z',\n",
       " 'endTime': '2019-12-12T21:35:59Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '10',\n",
       "  'trials': [{'trialId': '10',\n",
       "    'hyperparameters': {'alpha': '0.0018003200332565434', 'max_iter': '500'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6969913430957362},\n",
       "    'startTime': '2019-12-12T21:27:33.378290700Z',\n",
       "    'endTime': '2019-12-12T21:35:07Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '9',\n",
       "    'hyperparameters': {'alpha': '0.0023256017517083918', 'max_iter': '500'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.693691855026617},\n",
       "    'startTime': '2019-12-12T21:19:50.113970681Z',\n",
       "    'endTime': '2019-12-12T21:26:51Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '7',\n",
       "    'hyperparameters': {'alpha': '0.0022987899974855593', 'max_iter': '1000'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.693011548208242},\n",
       "    'startTime': '2019-12-12T21:19:47.133913295Z',\n",
       "    'endTime': '2019-12-12T21:27:08Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '4',\n",
       "    'hyperparameters': {'max_iter': '1000', 'alpha': '0.0028408783888816836'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6910046430940354},\n",
       "    'startTime': '2019-12-12T21:12:00.837478953Z',\n",
       "    'endTime': '2019-12-12T21:19:01Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '3',\n",
       "    'hyperparameters': {'alpha': '0.0034299265074729923', 'max_iter': '500'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6885555385478851},\n",
       "    'startTime': '2019-12-12T21:03:13.896174867Z',\n",
       "    'endTime': '2019-12-12T21:10:34Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '8',\n",
       "    'hyperparameters': {'alpha': '0.0033116740040831341', 'max_iter': '1000'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6878582240590506},\n",
       "    'startTime': '2019-12-12T21:19:50.112005405Z',\n",
       "    'endTime': '2019-12-12T21:26:41Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '6',\n",
       "    'hyperparameters': {'max_iter': '500', 'alpha': '0.0053383783888816833'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6855281732061159},\n",
       "    'startTime': '2019-12-12T21:12:00.837708006Z',\n",
       "    'endTime': '2019-12-12T21:19:31Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '1',\n",
       "    'hyperparameters': {'alpha': '0.0064815097784996032', 'max_iter': '1000'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6840825212170689},\n",
       "    'startTime': '2019-12-12T21:03:13.895875176Z',\n",
       "    'endTime': '2019-12-12T21:10:35Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '5',\n",
       "    'hyperparameters': {'alpha': '0.007835878388881683', 'max_iter': '500'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6832151300236406},\n",
       "    'startTime': '2019-12-12T21:12:00.837641245Z',\n",
       "    'endTime': '2019-12-12T21:18:31Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '2',\n",
       "    'hyperparameters': {'max_iter': '500', 'alpha': '0.0084249265074729917'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6820075854210249},\n",
       "    'startTime': '2019-12-12T21:03:13.896092793Z',\n",
       "    'endTime': '2019-12-12T21:10:46Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.64,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 'QeL/wtfME6Q='}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completedTrialCount': '10',\n",
       " 'trials': [{'trialId': '10',\n",
       "   'hyperparameters': {'alpha': '0.0018003200332565434', 'max_iter': '500'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6969913430957362},\n",
       "   'startTime': '2019-12-12T21:27:33.378290700Z',\n",
       "   'endTime': '2019-12-12T21:35:07Z',\n",
       "   'state': 'SUCCEEDED'},\n",
       "  {'trialId': '9',\n",
       "   'hyperparameters': {'alpha': '0.0023256017517083918', 'max_iter': '500'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.693691855026617},\n",
       "   'startTime': '2019-12-12T21:19:50.113970681Z',\n",
       "   'endTime': '2019-12-12T21:26:51Z',\n",
       "   'state': 'SUCCEEDED'},\n",
       "  {'trialId': '7',\n",
       "   'hyperparameters': {'alpha': '0.0022987899974855593', 'max_iter': '1000'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.693011548208242},\n",
       "   'startTime': '2019-12-12T21:19:47.133913295Z',\n",
       "   'endTime': '2019-12-12T21:27:08Z',\n",
       "   'state': 'SUCCEEDED'},\n",
       "  {'trialId': '4',\n",
       "   'hyperparameters': {'max_iter': '1000', 'alpha': '0.0028408783888816836'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6910046430940354},\n",
       "   'startTime': '2019-12-12T21:12:00.837478953Z',\n",
       "   'endTime': '2019-12-12T21:19:01Z',\n",
       "   'state': 'SUCCEEDED'},\n",
       "  {'trialId': '3',\n",
       "   'hyperparameters': {'alpha': '0.0034299265074729923', 'max_iter': '500'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6885555385478851},\n",
       "   'startTime': '2019-12-12T21:03:13.896174867Z',\n",
       "   'endTime': '2019-12-12T21:10:34Z',\n",
       "   'state': 'SUCCEEDED'},\n",
       "  {'trialId': '8',\n",
       "   'hyperparameters': {'alpha': '0.0033116740040831341', 'max_iter': '1000'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6878582240590506},\n",
       "   'startTime': '2019-12-12T21:19:50.112005405Z',\n",
       "   'endTime': '2019-12-12T21:26:41Z',\n",
       "   'state': 'SUCCEEDED'},\n",
       "  {'trialId': '6',\n",
       "   'hyperparameters': {'max_iter': '500', 'alpha': '0.0053383783888816833'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6855281732061159},\n",
       "   'startTime': '2019-12-12T21:12:00.837708006Z',\n",
       "   'endTime': '2019-12-12T21:19:31Z',\n",
       "   'state': 'SUCCEEDED'},\n",
       "  {'trialId': '1',\n",
       "   'hyperparameters': {'alpha': '0.0064815097784996032', 'max_iter': '1000'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6840825212170689},\n",
       "   'startTime': '2019-12-12T21:03:13.895875176Z',\n",
       "   'endTime': '2019-12-12T21:10:35Z',\n",
       "   'state': 'SUCCEEDED'},\n",
       "  {'trialId': '5',\n",
       "   'hyperparameters': {'alpha': '0.007835878388881683', 'max_iter': '500'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6832151300236406},\n",
       "   'startTime': '2019-12-12T21:12:00.837641245Z',\n",
       "   'endTime': '2019-12-12T21:18:31Z',\n",
       "   'state': 'SUCCEEDED'},\n",
       "  {'trialId': '2',\n",
       "   'hyperparameters': {'max_iter': '500', 'alpha': '0.0084249265074729917'},\n",
       "   'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6820075854210249},\n",
       "   'startTime': '2019-12-12T21:03:13.896092793Z',\n",
       "   'endTime': '2019-12-12T21:10:46Z',\n",
       "   'state': 'SUCCEEDED'}],\n",
       " 'consumedMLUnits': 0.64,\n",
       " 'isHyperparameterTuningJob': True,\n",
       " 'hyperparameterMetricTag': 'accuracy'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['trainingOutput']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_index = np.argmax([trial['finalMetric']['objectiveValue'] for trial in response['trainingOutput']['trials']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'trainingStep': '1', 'objectiveValue': 0.6969913430957362},\n",
       " {'alpha': '0.0018003200332565434', 'max_iter': '500'})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(trial['finalMetric'], trial['hyperparameters']) for trial in response['trainingOutput']['trials']][best_run_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1, 2, 3, 5, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM \n",
    "  `{{ source_table }}` AS cover\n",
    "WHERE \n",
    "  MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) in {{ lots_to_select }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = Template(sampling_query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID),\n",
    "    num_lots=10,\n",
    "    lots_to_select='(8)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None,\n",
    "    url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "    \n",
    "bigquery_query_op = component_store.load_component('bigquery/query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gcs_path = '{}/{}'.format(LAB_GCS_BUCKET, 'sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl.types import GCPProjectID\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_train(\n",
    "    project_id:GCPProjectID =PROJECT_ID,\n",
    "    query:str =query,\n",
    "    table_id:str =TRAINING_TABLE_ID,\n",
    "    dataset_id:str =DATASET_ID,\n",
    "    dataset_location:str =DATASET_LOCATION,\n",
    "    output_gcs_path:str =output_gcs_path\n",
    "    ):\n",
    "    \n",
    "    \n",
    "    \n",
    "    sample_data = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id=table_id,\n",
    "        output_gcs_path=output_gcs_path,\n",
    "        dataset_location=dataset_location\n",
    "        )\n",
    "    \n",
    "    from kfp.gcp import use_gcp_secret\n",
    "    kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_yaml = 'covertype_training.yaml'\n",
    "kfp.compiler.Compiler().compile(covertype_train, pipeline_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Pipeline link <a href=/pipeline/#/pipelines/details/25a8910c-441f-4d22-88fa-c81b4ed4eaff>here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_name = 'covertype_training_pipeline'\n",
    "client = kfp.Client()\n",
    "\n",
    "pipelines = [pipeline for pipeline in client.list_pipelines(page_size=100).pipelines if pipeline.name == pipeline_name]\n",
    "\n",
    "if pipelines:\n",
    "    print(\"Pipeline with this name already exists\")\n",
    "    pipeline_ref = pipelines[0]\n",
    "    \n",
    "else:\n",
    "    pipeline_ref = client.upload_pipeline(pipeline_yaml, pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfx",
   "language": "python",
   "name": "tfx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
