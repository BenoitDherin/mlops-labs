{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using custom containers with AI Platform Training\n",
    "\n",
    "In this lab, you develop, package as a docker image, and run on **AI Platform Training** a training application that trains a multi-class classification model that predicts the type of forest cover from cartographic data. The [dataset](../datasets/covertype/README.md) used in the lab is based on **Covertype Data Set** from UCI Machine Learning Repository.\n",
    "\n",
    "The training code uses **scikit-learn** for data pre-processing and modeling. The code has been instrumented using the `hypertune` package so it can be used with **AI Platform** hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings\n",
    "\n",
    "Create a GCS bucket that will be used as a staging area during the lab. Make sure to update the `PROJECT_ID` variable with your ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set location paths, connections strings, and other environment settings. Make sure to update project id and region with your settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'mlops-workshop'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "DATA_ROOT = 'gs://workshop-datasets/covertype'\n",
    "TRAINING_FILE_PATH = DATA_ROOT + '/training/data.csv'\n",
    "VALIDATION_FILE_PATH = DATA_ROOT + '/validation/data.csv'\n",
    "TESTING_FILE_PATH = DATA_ROOT + '/testing/data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bucket for AI Platform job directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mlops-workshop-lab11/...\n",
      "ServiceException: 409 Bucket mlops-workshop-lab11 already exists.\n"
     ]
    }
   ],
   "source": [
    "JOB_DIR_BUCKET = 'gs://{}-lab11'.format(PROJECT_ID)\n",
    "\n",
    "!gsutil mb $JOB_DIR_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Covertype dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3135</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>5</td>\n",
       "      <td>306</td>\n",
       "      <td>219</td>\n",
       "      <td>238</td>\n",
       "      <td>156</td>\n",
       "      <td>2790</td>\n",
       "      <td>Neota</td>\n",
       "      <td>7201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3211</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>5286</td>\n",
       "      <td>219</td>\n",
       "      <td>237</td>\n",
       "      <td>155</td>\n",
       "      <td>780</td>\n",
       "      <td>Rawah</td>\n",
       "      <td>7201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3046</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "      <td>666</td>\n",
       "      <td>218</td>\n",
       "      <td>238</td>\n",
       "      <td>156</td>\n",
       "      <td>1298</td>\n",
       "      <td>Rawah</td>\n",
       "      <td>7201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3211</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>437</td>\n",
       "      <td>30</td>\n",
       "      <td>5878</td>\n",
       "      <td>219</td>\n",
       "      <td>238</td>\n",
       "      <td>157</td>\n",
       "      <td>2230</td>\n",
       "      <td>Rawah</td>\n",
       "      <td>7201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3283</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>511</td>\n",
       "      <td>25</td>\n",
       "      <td>6031</td>\n",
       "      <td>218</td>\n",
       "      <td>238</td>\n",
       "      <td>157</td>\n",
       "      <td>631</td>\n",
       "      <td>Rawah</td>\n",
       "      <td>7201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173573</th>\n",
       "      <td>3147</td>\n",
       "      <td>96</td>\n",
       "      <td>59</td>\n",
       "      <td>216</td>\n",
       "      <td>-6</td>\n",
       "      <td>3037</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1209</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>7756</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173574</th>\n",
       "      <td>2722</td>\n",
       "      <td>172</td>\n",
       "      <td>60</td>\n",
       "      <td>124</td>\n",
       "      <td>55</td>\n",
       "      <td>2823</td>\n",
       "      <td>169</td>\n",
       "      <td>175</td>\n",
       "      <td>59</td>\n",
       "      <td>6480</td>\n",
       "      <td>Rawah</td>\n",
       "      <td>7746</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173575</th>\n",
       "      <td>2736</td>\n",
       "      <td>174</td>\n",
       "      <td>61</td>\n",
       "      <td>120</td>\n",
       "      <td>69</td>\n",
       "      <td>2853</td>\n",
       "      <td>163</td>\n",
       "      <td>171</td>\n",
       "      <td>59</td>\n",
       "      <td>6450</td>\n",
       "      <td>Rawah</td>\n",
       "      <td>7746</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173576</th>\n",
       "      <td>2500</td>\n",
       "      <td>360</td>\n",
       "      <td>61</td>\n",
       "      <td>255</td>\n",
       "      <td>81</td>\n",
       "      <td>569</td>\n",
       "      <td>58</td>\n",
       "      <td>53</td>\n",
       "      <td>74</td>\n",
       "      <td>1473</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173577</th>\n",
       "      <td>2506</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>201</td>\n",
       "      <td>88</td>\n",
       "      <td>655</td>\n",
       "      <td>73</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1470</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>4703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173578 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0            3135     135      0                               192   \n",
       "1            3211      90      0                                30   \n",
       "2            3046       0      0                               228   \n",
       "3            3211     180      0                               437   \n",
       "4            3283     225      0                               511   \n",
       "...           ...     ...    ...                               ...   \n",
       "173573       3147      96     59                               216   \n",
       "173574       2722     172     60                               124   \n",
       "173575       2736     174     61                               120   \n",
       "173576       2500     360     61                               255   \n",
       "173577       2506      13     64                               201   \n",
       "\n",
       "        Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                    5                              306   \n",
       "1                                    1                             5286   \n",
       "2                                    1                              666   \n",
       "3                                   30                             5878   \n",
       "4                                   25                             6031   \n",
       "...                                ...                              ...   \n",
       "173573                              -6                             3037   \n",
       "173574                              55                             2823   \n",
       "173575                              69                             2853   \n",
       "173576                              81                              569   \n",
       "173577                              88                              655   \n",
       "\n",
       "        Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0                 219             238            156   \n",
       "1                 219             237            155   \n",
       "2                 218             238            156   \n",
       "3                 219             238            157   \n",
       "4                 218             238            157   \n",
       "...               ...             ...            ...   \n",
       "173573            220               0              0   \n",
       "173574            169             175             59   \n",
       "173575            163             171             59   \n",
       "173576             58              53             74   \n",
       "173577             73              30              0   \n",
       "\n",
       "        Horizontal_Distance_To_Fire_Points Wilderness_Area  Soil_Type  \\\n",
       "0                                     2790           Neota       7201   \n",
       "1                                      780           Rawah       7201   \n",
       "2                                     1298           Rawah       7201   \n",
       "3                                     2230           Rawah       7201   \n",
       "4                                      631           Rawah       7201   \n",
       "...                                    ...             ...        ...   \n",
       "173573                                1209       Commanche       7756   \n",
       "173574                                6480           Rawah       7746   \n",
       "173575                                6450           Rawah       7746   \n",
       "173576                                1473       Commanche       4703   \n",
       "173577                                1470       Commanche       4703   \n",
       "\n",
       "        Cover_Type  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                2  \n",
       "4                1  \n",
       "...            ...  \n",
       "173573           2  \n",
       "173574           2  \n",
       "173575           2  \n",
       "173576           2  \n",
       "173577           2  \n",
       "\n",
       "[173578 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173578, 13)\n",
      "(58797, 13)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the training application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the `sklearn` training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all numeric features in training and validation datasets to Pandas `float64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_type_map = {feature: 'float64' for feature in numeric_features}\n",
    "\n",
    "df_train = df_train.astype(num_features_type_map)\n",
    "df_validation = df_validation.astype(num_features_type_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the pipeline locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', StandardScaler(copy=True, with_mean=True, with_std=True), ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Di...m_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.drop('Cover_Type', axis=1)\n",
    "y_train = df_train['Cover_Type']\n",
    "X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "y_validation = df_validation['Cover_Type']\n",
    "\n",
    "pipeline.set_params(classifier__alpha=0.001, classifier__max_iter=200)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the trained model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7036413422453527\n"
     ]
    }
   ],
   "source": [
    "accuracy = pipeline.score(X_validation, y_validation)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the hyperparameter tuning application.\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on AI Platform Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the tuning script. \n",
    "\n",
    "Notice the use of the `hypertune` package to report the `accuracy` optimization metric to AI Platform hyperparameter tuning service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "    \n",
    "  if not hptune:\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "  numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points']\n",
    "    \n",
    "  categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "    \n",
    "  num_features_type_map = {feature: 'float64' for feature in numeric_features}\n",
    "  df_train = df_train.astype(num_features_type_map)\n",
    "  df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  \n",
    "  if hptune:\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    # Log it with hypertune\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  if not hptune:\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "Notice the use of `mlops-dev:TF115-TFX015-KFP136` as a base image for the training image. The reason is to make sure that the development environment (your AI Platform Notebook instance) and the AI Platform Training environment are consistent. Since the AI Platform Notebook instance is based on the `mlops-dev:TF115-TFX015-KFP136` image we use this image as a base for the training image. \n",
    "\n",
    "Make sure to update the URI for the base image so that it points to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/mlops-workshop/mlops-dev:TF115-TFX015-KFP136\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the docker image. \n",
    "\n",
    "You use **Cloud Build** to build the image and push it your project's **Container Registry**. As you use the remote cloud service to build the image, you don't need a local installation of Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 3.1 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://mlops-workshop_cloudbuild/source/1579673215.41-9603be010533419f8a11aae8991689d8.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-workshop/builds/3dc000a4-4a05-4cc3-8108-6aac58d59e14].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/3dc000a4-4a05-4cc3-8108-6aac58d59e14?project=745302968357].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"3dc000a4-4a05-4cc3-8108-6aac58d59e14\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-workshop_cloudbuild/source/1579673215.41-9603be010533419f8a11aae8991689d8.tgz#1579673215838515\n",
      "Copying gs://mlops-workshop_cloudbuild/source/1579673215.41-9603be010533419f8a11aae8991689d8.tgz#1579673215838515...\n",
      "/ [1 files][  1.5 KiB/  1.5 KiB]                                                \n",
      "Operation completed over 1 objects/1.5 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM gcr.io/mlops-workshop/mlops-dev:TF115-TFX015-KFP136\n",
      "TF115-TFX015-KFP136: Pulling from mlops-workshop/mlops-dev\n",
      "35c102085707: Pulling fs layer\n",
      "251f5509d51d: Pulling fs layer\n",
      "8e829fe70a46: Pulling fs layer\n",
      "6001e1789921: Pulling fs layer\n",
      "1259902c87a2: Pulling fs layer\n",
      "83ca0edf82af: Pulling fs layer\n",
      "a459cc7a0819: Pulling fs layer\n",
      "7de7778cb300: Pulling fs layer\n",
      "62e0a31a8af6: Pulling fs layer\n",
      "a7785d29f5ab: Pulling fs layer\n",
      "6b76a06da4d7: Pulling fs layer\n",
      "413905cedc93: Pulling fs layer\n",
      "a5d245cced6f: Pulling fs layer\n",
      "8c6be6aa5553: Pulling fs layer\n",
      "1d7154118978: Pulling fs layer\n",
      "1df8626a77b0: Pulling fs layer\n",
      "a44e745885de: Pulling fs layer\n",
      "42e2291ef541: Pulling fs layer\n",
      "dd41a5143d21: Pulling fs layer\n",
      "0e9699f7bcd3: Pulling fs layer\n",
      "747ebef4a5c6: Pulling fs layer\n",
      "dd0264bbe7ae: Pulling fs layer\n",
      "5ac7068c86ac: Pulling fs layer\n",
      "add9e6ecc9a4: Pulling fs layer\n",
      "6001e1789921: Waiting\n",
      "1259902c87a2: Waiting\n",
      "83ca0edf82af: Waiting\n",
      "a459cc7a0819: Waiting\n",
      "7de7778cb300: Waiting\n",
      "62e0a31a8af6: Waiting\n",
      "a7785d29f5ab: Waiting\n",
      "6b76a06da4d7: Waiting\n",
      "413905cedc93: Waiting\n",
      "a5d245cced6f: Waiting\n",
      "8c6be6aa5553: Waiting\n",
      "1d7154118978: Waiting\n",
      "1df8626a77b0: Waiting\n",
      "a44e745885de: Waiting\n",
      "42e2291ef541: Waiting\n",
      "dd41a5143d21: Waiting\n",
      "0e9699f7bcd3: Waiting\n",
      "747ebef4a5c6: Waiting\n",
      "dd0264bbe7ae: Waiting\n",
      "5ac7068c86ac: Waiting\n",
      "add9e6ecc9a4: Waiting\n",
      "251f5509d51d: Verifying Checksum\n",
      "251f5509d51d: Download complete\n",
      "8e829fe70a46: Verifying Checksum\n",
      "8e829fe70a46: Download complete\n",
      "6001e1789921: Verifying Checksum\n",
      "6001e1789921: Download complete\n",
      "35c102085707: Verifying Checksum\n",
      "35c102085707: Download complete\n",
      "a459cc7a0819: Verifying Checksum\n",
      "a459cc7a0819: Download complete\n",
      "83ca0edf82af: Verifying Checksum\n",
      "83ca0edf82af: Download complete\n",
      "62e0a31a8af6: Verifying Checksum\n",
      "62e0a31a8af6: Download complete\n",
      "a7785d29f5ab: Verifying Checksum\n",
      "a7785d29f5ab: Download complete\n",
      "6b76a06da4d7: Verifying Checksum\n",
      "6b76a06da4d7: Download complete\n",
      "413905cedc93: Verifying Checksum\n",
      "413905cedc93: Download complete\n",
      "a5d245cced6f: Verifying Checksum\n",
      "a5d245cced6f: Download complete\n",
      "8c6be6aa5553: Verifying Checksum\n",
      "8c6be6aa5553: Download complete\n",
      "1259902c87a2: Verifying Checksum\n",
      "1259902c87a2: Download complete\n",
      "1d7154118978: Verifying Checksum\n",
      "1d7154118978: Download complete\n",
      "1df8626a77b0: Verifying Checksum\n",
      "1df8626a77b0: Download complete\n",
      "a44e745885de: Verifying Checksum\n",
      "a44e745885de: Download complete\n",
      "dd41a5143d21: Verifying Checksum\n",
      "dd41a5143d21: Download complete\n",
      "42e2291ef541: Verifying Checksum\n",
      "42e2291ef541: Download complete\n",
      "747ebef4a5c6: Verifying Checksum\n",
      "747ebef4a5c6: Download complete\n",
      "0e9699f7bcd3: Verifying Checksum\n",
      "0e9699f7bcd3: Download complete\n",
      "dd0264bbe7ae: Verifying Checksum\n",
      "dd0264bbe7ae: Download complete\n",
      "5ac7068c86ac: Verifying Checksum\n",
      "5ac7068c86ac: Download complete\n",
      "add9e6ecc9a4: Verifying Checksum\n",
      "add9e6ecc9a4: Download complete\n",
      "35c102085707: Pull complete\n",
      "7de7778cb300: Verifying Checksum\n",
      "7de7778cb300: Download complete\n",
      "251f5509d51d: Pull complete\n",
      "8e829fe70a46: Pull complete\n",
      "6001e1789921: Pull complete\n",
      "1259902c87a2: Pull complete\n",
      "83ca0edf82af: Pull complete\n",
      "a459cc7a0819: Pull complete\n",
      "7de7778cb300: Pull complete\n",
      "62e0a31a8af6: Pull complete\n",
      "a7785d29f5ab: Pull complete\n",
      "6b76a06da4d7: Pull complete\n",
      "413905cedc93: Pull complete\n",
      "a5d245cced6f: Pull complete\n",
      "8c6be6aa5553: Pull complete\n",
      "1d7154118978: Pull complete\n",
      "1df8626a77b0: Pull complete\n",
      "a44e745885de: Pull complete\n",
      "42e2291ef541: Pull complete\n",
      "dd41a5143d21: Pull complete\n",
      "0e9699f7bcd3: Pull complete\n",
      "747ebef4a5c6: Pull complete\n",
      "dd0264bbe7ae: Pull complete\n",
      "5ac7068c86ac: Pull complete\n",
      "add9e6ecc9a4: Pull complete\n",
      "Digest: sha256:21288e16d6018b540904d207965a8a9edcaf0d12edfe99b9554ed0b97fafa516\n",
      "Status: Downloaded newer image for gcr.io/mlops-workshop/mlops-dev:TF115-TFX015-KFP136\n",
      " ---> e37922616f4d\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in 62ee2659c82e\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: six in /root/miniconda3/lib/python3.5/site-packages (from fire) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor in /root/miniconda3/lib/python3.5/site-packages (from fire) (1.1.0)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103543 sha256=57aeb275d1da30df6a45d54a79fafc897d3114946c56709fc986d56e695cf1cb\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3993 sha256=c9a47b653a5b9c9f6c9d727fb8b5bde53c2e6ec1fd8be2f085935e1d701243f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/ac/62/80b621f3fe2994f3f367a36123d8351d75e3ea5591b4a62c85\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: fire, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.2.1\n",
      "Removing intermediate container 62ee2659c82e\n",
      " ---> a216909d03d7\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 14b8e0a29537\n",
      "Removing intermediate container 14b8e0a29537\n",
      " ---> 12b24893a1cb\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 5bd20f9aa736\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in d7c8f605c470\n",
      "Removing intermediate container d7c8f605c470\n",
      " ---> 51e7a2fa4019\n",
      "Successfully built 51e7a2fa4019\n",
      "Successfully tagged gcr.io/mlops-workshop/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-workshop/trainer_image:latest\n",
      "The push refers to repository [gcr.io/mlops-workshop/trainer_image]\n",
      "24f4724ea66a: Preparing\n",
      "4ec7028274af: Preparing\n",
      "c6669de9dfd5: Preparing\n",
      "859ef61cc336: Preparing\n",
      "99797d3ad3eb: Preparing\n",
      "1ec050ca8e3d: Preparing\n",
      "43ee21626282: Preparing\n",
      "b875beebc5b1: Preparing\n",
      "3885d9501aa4: Preparing\n",
      "cc4198cb51b8: Preparing\n",
      "98a86238f88d: Preparing\n",
      "07a867e0ba2d: Preparing\n",
      "092c50747c65: Preparing\n",
      "d6fb36f9bda1: Preparing\n",
      "f36c7efe6784: Preparing\n",
      "1f3727b0e386: Preparing\n",
      "80824689ea9a: Preparing\n",
      "dae8971c1728: Preparing\n",
      "63b763e1ea3e: Preparing\n",
      "7d412b9c88ab: Preparing\n",
      "4019db0181d2: Preparing\n",
      "5a78197acff6: Preparing\n",
      "804e87810c15: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "1ec050ca8e3d: Waiting\n",
      "43ee21626282: Waiting\n",
      "b875beebc5b1: Waiting\n",
      "3885d9501aa4: Waiting\n",
      "cc4198cb51b8: Waiting\n",
      "98a86238f88d: Waiting\n",
      "07a867e0ba2d: Waiting\n",
      "092c50747c65: Waiting\n",
      "d6fb36f9bda1: Waiting\n",
      "f36c7efe6784: Waiting\n",
      "1f3727b0e386: Waiting\n",
      "80824689ea9a: Waiting\n",
      "dae8971c1728: Waiting\n",
      "63b763e1ea3e: Waiting\n",
      "7d412b9c88ab: Waiting\n",
      "4019db0181d2: Waiting\n",
      "5a78197acff6: Waiting\n",
      "804e87810c15: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "99797d3ad3eb: Layer already exists\n",
      "859ef61cc336: Layer already exists\n",
      "43ee21626282: Layer already exists\n",
      "1ec050ca8e3d: Layer already exists\n",
      "b875beebc5b1: Layer already exists\n",
      "cc4198cb51b8: Layer already exists\n",
      "3885d9501aa4: Layer already exists\n",
      "98a86238f88d: Layer already exists\n",
      "07a867e0ba2d: Layer already exists\n",
      "092c50747c65: Layer already exists\n",
      "d6fb36f9bda1: Layer already exists\n",
      "1f3727b0e386: Layer already exists\n",
      "f36c7efe6784: Layer already exists\n",
      "80824689ea9a: Layer already exists\n",
      "63b763e1ea3e: Layer already exists\n",
      "dae8971c1728: Layer already exists\n",
      "7d412b9c88ab: Layer already exists\n",
      "4019db0181d2: Layer already exists\n",
      "5a78197acff6: Layer already exists\n",
      "804e87810c15: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "24f4724ea66a: Pushed\n",
      "c6669de9dfd5: Pushed\n",
      "4ec7028274af: Pushed\n",
      "latest: digest: sha256:e9d3821f4b0117c5dd9dec7fa817520f9d6627047c3b196d0e36296f90682e93 size: 5979\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                    IMAGES                                         STATUS\n",
      "3dc000a4-4a05-4cc3-8108-6aac58d59e14  2020-01-22T06:06:55+00:00  3M25S     gs://mlops-workshop_cloudbuild/source/1579673215.41-9603be010533419f8a11aae8991689d8.tgz  gcr.io/mlops-workshop/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "\n",
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the AI Platform hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the hyperparameter configuration file. \n",
    "Recall that the training code uses **sklearn SGDClassifier**. The training application has been designed to accept two hyperparameters that control **SGDClassifier**:\n",
    "- Max iterations\n",
    "- Alpha\n",
    "\n",
    "The below file configures AI Platform hypertuning to run up to 6 trials on up to three nodes and to choose from two discrete values of `max_iter` and the linear range betwee 0.00001 and 0.001 for `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 6\n",
    "    maxParallelTrials: 3\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_iter\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          200,\n",
    "          500\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DOUBLE\n",
    "      minValue:  0.00001\n",
    "      maxValue:  0.001\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the hyperparameter tuning job.\n",
    "\n",
    "Use the `gcloud` command to start the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200122_061611] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200122_061611\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200122_061611\n",
      "jobId: JOB_20200122_061611\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}/{}\".format(JOB_DIR_BUCKET, \"jobs\", JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the job.\n",
    "\n",
    "You can monitor the job using GCP console or from within the notebook using `gcloud` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-01-22T06:16:13Z'\n",
      "etag: 06Gy49c8xoY=\n",
      "jobId: JOB_20200122_061611\n",
      "state: PREPARING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset_path=gs://workshop-datasets/covertype/training/data.csv\n",
      "  - --validation_dataset_path=gs://workshop-datasets/covertype/validation/data.csv\n",
      "  - --hptune\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 3\n",
      "    maxTrials: 6\n",
      "    params:\n",
      "    - discreteValues:\n",
      "      - 200.0\n",
      "      - 500.0\n",
      "      parameterName: max_iter\n",
      "      type: DISCRETE\n",
      "    - maxValue: 0.001\n",
      "      minValue: 1e-05\n",
      "      parameterName: alpha\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "  jobDir: gs://mlops-workshop-lab11/jobs/JOB_20200122_061611\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/mlops-workshop/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20200122_061611?project=mlops-workshop\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2FJOB_20200122_061611&project=mlops-workshop\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2020-01-22 06:16:12 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2020-01-22 06:16:13 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2020-01-22 06:16:13 +0000\tservice\t\tJob JOB_20200122_061611 is queued.\n",
      "INFO\t2020-01-22 06:16:22 +0000\tservice\t1\tWaiting for job to be provisioned.\n",
      "INFO\t2020-01-22 06:16:22 +0000\tservice\t3\tWaiting for job to be provisioned.\n",
      "INFO\t2020-01-22 06:16:22 +0000\tservice\t2\tWaiting for job to be provisioned.\n",
      "INFO\t2020-01-22 06:16:24 +0000\tservice\t1\tWaiting for training program to start.\n",
      "INFO\t2020-01-22 06:16:24 +0000\tservice\t2\tWaiting for training program to start.\n",
      "INFO\t2020-01-22 06:16:24 +0000\tservice\t3\tWaiting for training program to start.\n",
      "INFO\t2020-01-22 06:23:12 +0000\tmaster-replica-0\t3\tStarting training: alpha=0.0009933559858798981, max_iter=500\n",
      "INFO\t2020-01-22 06:23:12 +0000\tmaster-replica-0\t3\tModel accuracy: 0.703879449631784\n",
      "INFO\t2020-01-22 06:23:23 +0000\tmaster-replica-0\t2\tStarting training: alpha=0.0005203486084938049, max_iter=500\n",
      "INFO\t2020-01-22 06:23:23 +0000\tmaster-replica-0\t2\tModel accuracy: 0.7083524669626001\n",
      "INFO\t2020-01-22 06:24:07 +0000\tmaster-replica-0\t1\tStarting training: alpha=2.5348608493804933e-05, max_iter=500\n",
      "INFO\t2020-01-22 06:24:07 +0000\tmaster-replica-0\t1\tModel accuracy: 0.7130295763389288\n",
      "INFO\t2020-01-22 06:26:57 +0000\tservice\t3\tJob completed successfully.\n",
      "INFO\t2020-01-22 06:27:24 +0000\tservice\t2\tJob completed successfully.\n",
      "INFO\t2020-01-22 06:27:58 +0000\tservice\t4\tWaiting for job to be provisioned.\n",
      "INFO\t2020-01-22 06:27:59 +0000\tservice\t5\tWaiting for job to be provisioned.\n",
      "INFO\t2020-01-22 06:28:00 +0000\tservice\t4\tWaiting for training program to start.\n",
      "INFO\t2020-01-22 06:28:01 +0000\tservice\t5\tWaiting for training program to start.\n",
      "INFO\t2020-01-22 06:28:26 +0000\tservice\t1\tJob completed successfully.\n",
      "INFO\t2020-01-22 06:29:40 +0000\tservice\t6\tWaiting for job to be provisioned.\n",
      "INFO\t2020-01-22 06:29:41 +0000\tservice\t6\tWaiting for training program to start.\n",
      "INFO\t2020-01-22 06:34:54 +0000\tmaster-replica-0\t6\tStarting training: alpha=0.0004316999912261963, max_iter=200\n",
      "INFO\t2020-01-22 06:34:54 +0000\tmaster-replica-0\t6\tModel accuracy: 0.7091178121332721\n",
      "INFO\t2020-01-22 06:35:18 +0000\tmaster-replica-0\t4\tStarting training: alpha=0.0008401049911975861, max_iter=500\n",
      "INFO\t2020-01-22 06:35:18 +0000\tmaster-replica-0\t4\tModel accuracy: 0.7053080939503716\n",
      "INFO\t2020-01-22 06:36:11 +0000\tmaster-replica-0\t5\tStarting training: alpha=0.0007485227024555207, max_iter=500\n",
      "INFO\t2020-01-22 06:36:11 +0000\tmaster-replica-0\t5\tModel accuracy: 0.7061924928142592\n",
      "INFO\t2020-01-22 06:39:11 +0000\tservice\t6\tJob completed successfully.\n",
      "INFO\t2020-01-22 06:39:23 +0000\tservice\t4\tJob completed successfully.\n",
      "INFO\t2020-01-22 06:40:21 +0000\tservice\t5\tJob completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completes you can review the results using GCP Console or programatically by calling the AI Platform Training REST end-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned runs are sorted by the optimization metric. The best run is the first item on the returned list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://mlops-workshop-lab11/datasets/training/data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
